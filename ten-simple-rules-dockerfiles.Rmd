---
title: "Ten Simple Rules for Writing Dockerfiles for Reproducible Research"
author:
  - name: Daniel NÃ¼st
    email: daniel.nuest@uni-muenster.de
    affiliation: 'Institute for Geoinformatics, University of Muenster, Muenster, Germany'
    corresponding: daniel.nuest@uni-muenster.de
    orcid: 
  - name: Vanessa Sochat
    email: vsochat@stanford.edu
    affiliation: "Stanford Research Comuting Center, Stanford University, Stanford, CA, US"
    orcid: 0000-0002-4387-3819
  - name: Stephen Eglen
    email: sje30@cam.ac.uk
    affiliation: "Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, Cambridgeshire, GB"
    orcid: 0000-0001-8607-8025
  - name: Tim Head
    email: betatim@gmail.com
    affiliation: "Wild Tree Tech, Zurich, CH"
    orcid: 0000-0003-0931-
  - name: Tony Hirst
    email: tony.hirst@open.ac.uk
    affiliation: "Department of Computing and Communications, The Open University, GB"
    orcid: 0000-0001-6921-702X
abstract: |
  Containers are greatly improving computational science by packaging software and data dependencies.
  In a scholarly context, transparency and support of reproducibility are the largest drivers for using these containers.
  It follows that choices that are made with respect to building containers can make or break a worklow's reproducibility.
  The build for the container image is often created based on the instructions in a plain-text file.
  For example, one such container technology, Docker, provides instructions using a `Dockerfile`.
  By following the rules in this article researchers writing a `Dockerfile` can effectively build and distribute containers.
#author_summary: |
#  TBD
bibliography: bibliography.bib
output: rticles::plos_article
csl: plos.csl
# Submission notes:
# - The authors declare that no competing interests exist.
---

<!-- please stick to one sentence per line! -->
<!-- Citations: @Figueredo:2009dg, citep: [@Figueredo:2009dg], multiple citep: [@Figueredo:2009dg; @other_2018] -->
<!-- add " {-}" after headings to remove numbering -->

# Introduction {#introduction .unnumbered}

With access to version control systems and only collaboration platforms based on these, such as [GitHub](https://github.com) or [GitLab](https://gitlab.com), it has become increasingly easy to not only share algorithms, but also instructions for executing research workflows, including building and testing of the used software.
The publication of these artefacts are a response to the increasing complexity of computer-based research, which is too complicated for "papers" to fully communicate the knowledge [@marwick_how_2015] but where the actual knowledge is the full software environment that produced a result [@donoho_invitation_2010].
A research compendium is a compilation of data, code, and documentation that accompanies, or is itself, a scholarly publication for increased transparency and reproducibility (cf. [https://research-compendium.science](https://research-compendium.science)).
Within such a compendium, it is suggested to include instructions for building containers to package software and data dependencies [@boettiger_introduction_2017].
By way of providing these instructions along with thorough documentation and application of common practices for scientific computing [@wilson_best_2014; @wilson_good_2017], it's much more likely both the author and third parties are able to reproduce an analysis workflow.
These instructions are human and machine readable and the containers built from them are portable encapsulated snapshots of a specific environment.
For example, containers have been created to host scientific notebooks [@rule_ten_2019], and share reproducible workflows (cf. Rule 10 of @sandve_ten_2013).

While there are several tutorials for using containers for reproducible research [@nust_author_2017; @chapman_reproducible_2018; @ropensci_labs_r_2015; @udemy_docker_2019; @psomopoulos_lesson_2017], there is no extensive examination for how to write the actual instructions to create the containers.
Several platforms for facilitating reproducible research are built on top of containers [@brinckman_computing_2018; @code_ocean_2019; @simko_reana_2019; @jupyter_binder_2018; @nust_opening_2017], but they hide any complexity from the researcher.
While not everybody needs to understand and write Dockerfiles, as _"the number of unique research environments approximates the number of researchers"_ [@nust_opening_2017], the researchers who do need to craft their own environment definition should follow good practices.
Such practices are not part of generic Docker tutorials and are not present in published Dockerfiles often used as templates.
The differences between a helpful stable Dockerfile and one that is misleading, prone to failure, and contains potential obstacles, are not obvious, especially for researchers who don't have software development experience.

While there are many different container technologies, this article focuses on Docker [@wikipedia_contributors_docker_2019]. 
The goal at hand is to write a `Dockerfile` so that it best facilitates interactive development and research as well as the higher goals of reproducibility and preservation. 
A daily commitment to these general practices can ensure that workflows are reproducible, and generation of a container is not an afterthought triggered by a scientific publication (cf. thoughts on openness as an afterthought by @chen_open_2019 and on computational reproducibility by @donoho_invitation_2010).
By following the _conventions_ layed out in these ten rules, authors ensure readability by others and ideally subsequent reuse and collaboration.

## Docker {#docker .unnumbered}

Docker is a container technology that is widely adopted and supported on many platforms, and has become highly useful in science.
They are distinct from virtual machines (VM) or hypervisors as they do not emulate hardware, and thus do not require the same system resources.
To build Docker containers, we write text files that follow a particular format called `Dockerfile`s [@docker_inc_dockerfile_2019]. 
`Dockerfile`s are machine- and human-readable recipes for creating containers.
While Docker was the original technology to support this format, other container technologies have developed around the format (and thus support it) including: [podman](https://podman.io/)/[buildah](https://github.com/containers/buildah) supported by RedHat, [kaniko](https://github.com/GoogleContainerTools/kaniko), [img](https://github.com/genuinetools/img), and [buildkit](https://github.com/moby/buildkit).
The Singularity container software [@kurtzer_singularity_2017] is optimized for high performance computing and although it uses its own format, the _Singularity recipe_, it can import Docker containers directly from a Docker registry. 
Although the Singularity recipe format is different, the rules here are transferable to some extent.
While some may argue for reasons to not publish reproducibly (lack of time & incentives and unfittingness for a researcher's workflow, cf. [@boettiger_introduction_2015]) and there are substantial technical challenges to maintain software and documentation, providing a `Dockerfile` or pre-built Docker container should become an increasingly easier task for the average researcher.
If a researcher is able to find or create containers or `Dockerfile`s to address their most common use cases, then arguably it will not be extra work after this initial set up (cf. README of [@marwick_madjebebe_2015]). 
In fact, the `Dockerfile` itself can be used as documentation to clearly show from where data and code was derived, i.e. downloaded or installed, and consequently where a third party might obtain them again.
Importantly, the researcher should clearly state the platforms (by way of one or more Docker containers) have been tested on.

# 1. Consider tools to assist with Dockerfile generation {-}

Writing a Dockerfile from scratch is not that simple, and even "experts" sometimes take shortcuts.
Thus, it's a good strategy to first look to tools that can help to generate a Dockerfile for you.
Such tools have likely thought about and implemented good practices, and they may have added newer practices when reapplied at a later point in time.
It's always a good practice to start with your specific use case.
You would first want to determine if there is an already existing container that you can use, and in this case, use it and add to your workflow documentation instructions for doing so.
As an example, you might be doing some kind of interactive development.
For interactive development environments such as notebooks and development servers or databases, you can readily find containers that come installed with the software that you need. 
In the case that there isn't an existing container for your needs, you might next look to well-maintained tools to help with `Dockerfile` generation.
Well maintained means that recent container bases are used, likely from the official Docker library [@docker_inc_official_2019], to ensure that the container has the most recent security fixes for the operating system in question.
As an example, repo2docker [@jupyter_binder_2018] is a tool that is maintained by Jupyter Labs that can help to transform a repository in the format of some known kind of package (notebook, Python, R, etc.) into a container.
Such a package contains well-defined files for defining software dependencies and versions, which repo2docker can understand.
As an example, we might install `jupyter-repo2docker` and then run it against a repository with a `requirements.txt` file, an indication of being a Python package with the following command.

```bash
jupyter-repo2docker https://github.com/norvig/pytudes
```

The resulting container image would install the dependencies listed in the requirements file, along with providing an entrypoint to run a notebook server to easily interact with any existing workflows in the repository.
A precaution that needs to be taken is that the default command above will create a home for the current user, meaning that the container itself wouldn't be ideal to share, but rather any researchers interested in interaction with the code inside should build their own container.
For this reason, it's good practice to look at any help command provided by the tool and check for configuration options for user names, user ids, and similar.
It's also recommended (if you are able) to add custom labels to your container build to define metadata for your analyses, if relevant.
The container should be built to be optimized for its use case, whether it is intended to be shared or used by a single user.

Further assisting tools are `containerit` [@nust_containerit_2019] and `dockta` [@stencila_dockta_2019].
`containerit` automates the generation of standalone Dockerfiles for workflows in R.
It can provide a starting point for users unfamiliar with writing Dockerfiles, or together with other R packages provide a full image creation and execution process without having to leave an R session.
`dockta` supports multiple programming languages and configurations files, just as `repo2docker`, but attempts to create readable Dockerfiles compatible with plain Docker and to improve user experience by cleverly adjusting instructions to reduce build time.

However, in the case that a tool or interactive container environment is not available, or their capabilities to configure specific resources or install bespoke tools, you will likely need to write a `Dockerfile` from scratch.
In this case, proceed with following the remaining rules to write your `Dockerfile`.

# 2. Use versioned and automatically built base images {-}

It's good practice to use base images that are maintained by the Docker library.
While some organizations can be trustworthy in terms of content, and reliable to update containers with security fixes (e.g., `rocker/r-ver`), for most individual accounts that build containers, it's likely that containers will not be updated regularly.
It is even possible that images or Dockerfiles disappear.
Therefore a good understanding of how base images and image tags work is crucial, as the tag that you choose has implications for your container build.
You should know how to use `FROM` statements to trace all the way up to the original container base, an abstract base called `scratch`.
Doing this kind of trace is essential to be able to be aware of all of the steps that were taken to generate your container and who took them.
If you want to build on a third party image, carefully consider the author/maintainer and _never_ use an image without a published Dockerfile.
If you want to use an unofficial image, do save a copy of the Dockerfile(s) created by others.
Alternatively copy relevant instructions into your own Dockerfile, acknowledge the source in a comment, and configure an automated build for your own image.

A tag like `latest` is good in that security fixes and updates are likely present, however it is bad in that it's a moving target so that it is more likely that an updated base could break your workflow.
Other tags that should be avoided are `dev`, `devel`, or `nightly` that may provide possibly unstable versions of the software.
As an example, the Python container on Docker Hub will be released with new versions of Python.
If you build a container with `python:latest` that only supports Python version 3, when the base is updated to Python 4 it's likely that your software won't work as expected. 
In this case, a tag like `python:3.5` might be a good choice, where the third component of the version, i.e., `3.5.x`, ensures that security patches and bugfixes that won't break your code, i.e., changes that are backwards compatible, are applied (cf. semantic versioninig, [@preston-werner_semantic_2013]).
When you choose a base image, choose one with a Linux distribution that supports the software stack you are using, and also take into account the bases that are widely used by your community.
As an example, Ubuntu is heavily used for geospatial research, and so the `rocker/geospatial` image would be a good choice for spatial data science with R, or `jupyter/tensorflow-notebook` could be a good choice for machine learning with Python.
Especially when data is involved, containers can grow quickly in size, and you should keep this in mind.
If you need to built smaller containers, consider a `busybox` base or a multi-stage build, which allows to selectively keep files from one build step to another.
Smaller images often are indicated by having `slim` or `minimal` as part of the tag.
Also take into account the libraries that you actually need.
Base images that have complex software installed (e.g. machine learning libraries, specific BLAS library) are helpful and fine to use.
If you do not want to rely on a third party or other individual to maintain the recipe and container, you should copy the Dockerfile to your own repository, and also provide a deployment for it via your own automated build. 
Trusting that the third party `Dockerfile` will persist for your analyses is a risky thing to do.
For the new user, a potential list of existing communities with regular builds and updates includes:

  - [Rocker](https://www.rocker-project.org/) for R [@boettiger_introduction_2017]
  - [Docker containers for Bioconductor](https://bioconductor.org/help/docker/) for bioinformatics
  - [NeuroDebian images](https://hub.docker.com/_/neurodebian) for neuroscience [@halchenko_open_2012]
  - [Jupyter Docker Stacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html for Notebook-based computing
  - [Taverna Server](https://hub.docker.com/r/taverna/taverna-server) for running Taverna workflows

For example, here is how we would use a base image `r-ver` with tag `3.5.2` from the `rocker` organization on Docker Hub (`docker.io`)
```
FROM rocker/r-ver:3.5.2
```

# 3. Use formatting and favor clarity

It's good practice to think of the `Dockerfile` as being human _and_ machine readable. 
This means that you should use indentation, newlines, and comments to make your `Dockerfile`s well documented and give them readability.
Specifically, carefully indent commands and their arguments to make clear what belongs together, especially when connecting multiple commands in a `RUN` instruction with `&&`.
Use `\` to break a single command into multiple lines to ensure that no single line gets too long, and use long versions of parameters for readability (e.g., `--input` instead of `-i`).
When you need to change a directory, use `WORKDIR`, because it not only creates the directory if it doesn't exist but also persist the change across multiple `RUN` instructions.

As you are writing the `Dockerfile`, put yourself in the shoes of the next user of it.
Are the choices made and commands being executed clear, or is further comment warranted?
When appropriate, you can add links to discussion in version control, code repository issues, or comments about specific usage and decisions.
Dependencies can be grouped in this fashion, which also makes it easier to spot changes if your Dockerfile is under version control.
It can even be helpful to include comments about possibly more intuitive commands that did not work so you do not fall into the same trap twice.
If you find that you need to remember an undocumented step, that's an indication that it should be documented.

Labels are useful to provide a more structured form of documentation about software.
For example, software versions, maintainer contact information, along with vendor specific metadata are commonly seen and the OCI Image Format Specification provides some common label keys [@opencontainers_image-spec_2017].

Comments can also include usage guides, which can be helpful to others and a future you. 
As an example, it's helpful to provide commented lines with `docker build` and `docker run` within the `Dockerfile` to show how to build and run the image.
These comments can be especially relevant if volume mounts or ports are important for using the container, and by putting them at the end of the file, they are more likely to be seen, easy to copy paste after a container build, and to be in consistent state compared to documentation in another file.
Here is an example of a commented section to show build and usage.

```
# Build the images with
##> docker build --tag great_workflow .
# Run the image:
##> docker run --it --port 80:80 --volume ./input:/input --name gwf great_workflow
# Extract the data:
##> docker cp gwf:/output/ ./output
```

If you were to discover a previously written `Dockerfile` and not remember the container identifier you used, it would be represented in the `Dockerfile` in the `--name` CLI parameter.
Following a common coding aphorism, we might say _"A Dockerfile written three months ago may just as well have been written by someone else"_.
Here is an example of various kinds of comments that might be useful for a `Dockerfile`:

```
# apt-get install specific version, use 'apt-cache madison <pkg>' to see available versions
RUN apt-get install python3-pandas=0.23.3+dfsg-4ubuntu1

# RUN command spreading several lines
RUN R -e 'getOption("repos")' && \
  install2.r \
    fortunes \
    here

# this library must be installed from source to get version newer than in sources

# following commands from instructions at LINK HERE
```

Clarity is always more important than brevity.
For example, if your container uses a script to run a complex install routine, instead of removing it from the container upon completion, which is commonly seen in production Dockerfiles aiming at small image size, you should keep the script in the container for a future user to inspect.
Generally, try to design the `RUN` statements so that each performs one scoped action (e.g., download, compile, and install one tool).

If a `RUN` statement is longer than a page and requires scrolling, this may be challenging for the next reader to digest, and you should consider splitting it up into several.
When you install several system libraries, it's good practice to add comments about why the dependencies are needed. 
This way, if a piece of software is removed from the container, it will be easier to remove the system dependencies that are no longer needed.
If you intend to build the image more than once (perhaps during development) and you can take advantage of build caching to avoid execution of time-consuming instructions, e.g., install from a remote resource or a file that gets cached.
You should list commands _in order_ of least likely to change to most likely to change and use the `--no-cache` flag to force a re-build of all layers.
A recommended ordering based on this metric might be:

  1. system libraries
  2. language-specific libraries or modules
    1. from repositories (binaries)
    2. from source
  3. own software/scripts (if not mounted)
  4. labels
  5. `RUN`/`ENTRYPOINT`

Finally, as a supplement to content inside the `Dockerfile`, it's good practice to also write a section in a README alongside the Dockerfile for exactly how to build, run, and otherwise interact with the container. 
If a pre-built image is provided on Docker Hub, you should direct the user to it.

<!-- not sure how the below fits in, moving down here for someone else to address-->
- modularisation is a two-edged sword (point out [`podman`'s `#include` directive](https://www.mankier.com/1/podman-build)?) <!--I don't understand this point-->
- use a linter to follow common practices and consistency <!-- suggestion?? What linter do you have in mind?-->


# 4. Pin versions {-}

The reproducibility of your `Dockerfile` is heavily dependent on how well you define versions for dependencies inside.

**system libraries**

System library versions can largely come from the base image tag that you choose to use (e.g., `ubuntu:18.04`) however, you can also install specific versions of system packages with the respective package manager. 
This practice is called version pinning (e.g., on apt: https://blog.backslasher.net/my-pinning-guidelines.html).
Version pinning is good practice if the version is relevant.
For example, you might want to demonstrate a bug, prevent a bug in an updated version, or pin a working version if you suspect an update could lead to a problem.
Generally, system libraries are more stable than software modules supporting analysis scripts, but in some cases they can be highly relevant to your workflow.
_Installing from source_ is a useful way to install very specific versions, however it comes at the cost of needing build libraries.
Here is how you might go about listing the currently installed versions of software on your system:

  - Debian/Ubuntu: `dpkg --list`
  - Alpine: `apk -vv info|sort`
  - CentOS: `yum list installed` or `rpm -qa`

**extension packages and programming language modules**

In the case of needing to install packages or dependencies for a specific language, package managers are a good option.
Package managers generally provide reliable mirrors or endpoints to download software, and many packages are tested before release.
Most package managers have a client (CLI) interface that can easily be used from `RUN` commands in your `Dockerfile`, along with various flavors of "freeze" commands that can output a text file listing all software packages and versions (cf. https://markwoodbridge.com/2017/03/05/jupyter-reproducible-science.html cited by @rule_ten_2019)
The biggest risk with using package managers with respect to `Dockerfile`s is outsourcing configuration to file formats that are supported. 
As an example, here are configuration files supported by commonly used languages in scientific programming:

  - Python: `requirements .txt`, `xxx.yml` (Conda)
  - R: `DESCRIPTION`
  - Java: `mvn.xml`
  - JavaScript: `package.json` of `npm`

In all of the above, the user would be required to inspect the file or the build to see what is installed. 
In some cases (e.g., conda) the package manager is also able to make decisions about what versions to install, which is likely to lead to a non-reproducible build.
For this reason, in the case of having few packages, it is suggested to write the install steps and versions directly into the `Dockerfile`.
For example, the `RUN` statement here:

```bash
RUN pip install geopy==1.20.0 && \
    pip install uszipcode==0.2.2
```
serves as more clear documentation in a `Dockerfile` than a requirements.txt file that lists the same:

```bash
RUN pip install -r requirements.txt
```

<!--Note from @vsoch - I think having the formats for each is out of scope, maybe instead just link to documentation for each, we wouldn't do good justce to explain in detail here-->
- how to do in Python (`== x.y.z`)
- do it in R with `versions` package, or by using MRAN (e.g. via `r-ver` image)- JavaScript?
- Julia: `add Package@1.0` > https://julialang.github.io/Pkg.jl/v1/managing-packages/#Adding-packages-1
- Use common command-line ready installation commands of programming languages
  - better readbiliy, potentially even performance (`RUN install2.r sp` instead of `RUN R -e "install.packages('sf')"`, although the latter is "base R")

  
# 5. Mount data and control code {-}

The addition of data or code into the container using `ADD`/`COPY` is highly dependent on your use case. 
For example, small data files and software that is essential for the function of a container to distribute a reproducible analysis is essential to add. 
However, if the container is intended to be used as an interactive environment, you should refrain from adding it to the container, and instead bind the files from the outside of the container. 
This also ensures that data persists when the container instance or image is removed from your system. 
When you do this, in the case that you will likely save as a root user from inside the container, you should be careful about file permissions. 
To avoid problems when mounting, Docker has provided a `--user` option that you should use. 
Without this option, there can be unexpected issues with permissions. 
For example, writing a new file from inside the container will be owned by user "root" on your host. In that many applications generate log and other output files during runtime, be cautious about what volumes are bound to the host.
The other logical case to bind mount code is when your data is very large (usually over hundreds of GB) or has limited access. 
For example, a private dataset with personal health information (PHI) should never be added to a container. 
In these cases, you should provide clear instructions to the user in the README for how to obtain actual or dummy data.

<!-- Not sure how these fit in-->
- easier access, does not require Docker knowledge by third parties to reause code and data
- if you have a "stable" published software library, install it from source from the source code repo or from the software repository (so that users find the project in the future)
- how to mount the data can be put in the example commands (see 4.)

```
docker run --volume ...
```

<!-- End not sure how these fit in -->

# 6. Capture environment metadata {-}

Labels and build arguments can be very helpful to both provide metadata and allow for customization of a build.

- **labels**

Labels serve as structured metadata that can be exposed by APIs (e.g., https://microbadger.com/labels) along with tools to inspect the container binaries (e.g., `docker inspect`).
Namespaced conventions have been developed (http://label-schema.org/rc1/ respectively https://github.com/opencontainers/image-spec) to help standardize across container tools.
Important metadata attributes to include as labels would include any of the following:

 - Research organizations (https://ror.org/)
 - Funding agency/grant number
 - DOI of research compendium
 - Author DOIs (e.g., Zenodo or other)
 - repository link where Dockerfile is published
 - author (`MAINTAINER` is deprecated) and contact (e.g. email, project website)
 - license

There are several projects (https://codemeta.github.io/, https://citation-file-format.github.io/) focused around metadata citation, and you should consider using one if it seems appropriate for your use case.

- **build arguments**

Build arguments can provide metadata and also allow for customization of a build.
As an example, the following build argument would default to `1.0.0` but allow the user to change it with `--build-arg MYVERSION=2.0.0`

```
ARG MYVERSION=1.0.0
```

Along with specifying versions, environment, or timestamp, build arguments can be useful to provide the context of the build (e.g., production, development, automated or not).
Examples of build arguments that are useful to include to describe a container are:

  - git commit hash
  - date and timestamp


# 7. Enable interactive usage and one-click execution {-}

The success of a container can sometimes come down to how well it exposes its usage and software to the user: a function that comes down to the `CMD` and `ENTRYPOINT`.
By using `CMD` and `ENTRYPOINT`, you can use automated and manual testing to ensure that it is possible to run the container interactively _and_ given that execution is done incorrectly or has an error, a suitable help or error message is shown to assist the user.
A possible weakness with using containers is the limitation on only providing one entrypoint, however tools (e.g., The Scientific Filesystem REF academic.oup.com/gigascience/article/7/5/giy023/4931737) have been developed to expose multiple entrypoints, environments, help messages, labels, and even install sequences.
It's considered good practice to have an entrypoint that meets the user expectations. 
For example, a container known to be a workflow should execute the workflow, or spit out usage for how to do so.
An interactive container should spin up an analysis environment, ensuring to print to the screen the ports that are being used (that need to be bound to the host) along with any login credentials needed.
A workflow that does not support headless execution is arguably not ideal for a container, and you should consider switching tools. 
Using a browser to expose a user interface (e.g, RStudio, Jupyter, web tools) on a particular port is a commonly done practice, and for systems without native `x11` support, `x11docker` is recommended [REF].
Otherwise, you would need to clearly document how the user can start and control the graphical interfaces provided inside.
However, headless execution is ideal in that it can be tested headlessly, either by starting or interacting with the container and checking for successful responses (e.g., 200) from endpoints provided, or by using a controller such as Selenium [REF].
To support both "one click execution" and to allow for custom configuration, it's helpful to provide users with a configuration file, or allow for settings to be expressed via environment variables  [@knoth_reproducibility_2017], or special Docker-based wrappers such as Kliko [@molenaar_klikoscientific_2018]

<!-- This is out of scope for this section, and redundant, I would remove it-->
- may also use the same `Dockerfile` for different purposes, e.g. include an app (e.g. Shiny) for interactive use by user
- see also Rule 7: Build a pipeline in @rule_ten_2019
- _what user should run within the Dockerfile?_ 
<!-- End out of scope-->

<!-- SUGGESTION for rule 8 instead of redundant comments with build commands - be about having a workflow to start a new project-->
# 8. Establish templates for new projects

It's likely going to be the case that over time you will develop interactive environments, servers, or workflows that are similar in nature to each other.
In this case, you should consider adopting a standard workflow that will give you a clean slate for a new project.
As an example, cookie cutter templates (e.g., https://github.com/cookiecutter/cookiecutter) and project starter kits can provide files, directory organization, and build instructions that include basic steps for getting started.
A good project template should get you started with a template for documentation, setting up testing via continuous integration, building a container, and even choosing a license.
In the case of using a common `Dockerfile` or base, Docker's build caching will take shared lines into account and speed up build time.
It also must be noted that developing or working on projects with containers, period, allows you to easily switch between isolated project environments.
At most, if a port is shared by two projects, you would simply need to stop the container and restart when you are ready to work again.
In the case of more complex web applications that require application and web servers, databases, and workers or messenging, the entire infrastructure can easily be brought up or down with a tool like `docker-compose` [REF].
`docker-compose` also allows definition of services via it's own `docker-compose.yml`, ranging from mounted volumes, to permissions, environment, and ports.
Part of this project template should be a protocol for publishing the container image to a suitable container registry, and taking into consideration of how the code can be given a DOI or proper publication (e.g., Zenodo, Journal of Open Source Software).

# 9. Publish a Dockerfile per project in a code repository with version control {-}

The `Dockerfile` is a plain text-based format and thus fits nicely into a version control system.
You should tend to include `Dockerfile`s alongside your code as a way to quickly build your software, and show visitors of the repository how it's built (qoute Abelson? see also https://www.quora.com/How-true-is-Programs-are-meant-to-be-read-by-humans-and-only-incidentally-for-computers-to-execute)/, and as mentioned previously, the `Dockerfile` itself can then link back to the repository.
Take advantage of git commit messages to keep a record of notes alongside changes to your files.
Versioning and publishing on a collaboration platform is also beneficial in that is enables collaboration and makes it easy to solicit feedback.
Version control providers (e.g., GitHub, GitLab) also make it easy to connect to or use embedded continuous integration (CI) services to test and deploy your containers and software.
While it's not always possible to keep datasets alongside code, in the case of smaller ones, it can be very useful to keep your `Dockerfile` alongside both the code and data (cf. research compendium concept?).
While there are exceptions to the rule (cf. @kim_bio-docklets_2017), it's generally a simple and clear approach to provide one Dockerfile per project.
If you find that you need to provide more than one, consider if it's possible to use build arguments to flip between states (e.g., development vs. production) or if it's possible to separate tools into different repositories.
In fact, using and publishing a `Dockerfile` to create a container has been shown to increase chances of preservation of your work (cf. @emsley_framework_2018)

<!-- Not sure how these fit in-->
Consult Ten Simple Rules paper by Perez-Riverol et al. [@perez-riverol_ten_2016]

# 10. Use the container daily, rebuild the image weekly {-}

A container that is not regularly rebuilt risks not getting essential security updates, and serving as a risk for its users.
If you are using a container built by a `Dockerfile` regularity in your work, it's recommended to set up continuous deployment, or a strategy to ensure that the container is regularly built and tested.
While there is no hardened rule, it would be good practice to rebuild a container every once or two weeks that you use on a daily basis.
In an idealistic world, containers that we built years ago should rebuild seamlessly, but this isn't the case, especially with rapidly changing technology relevant to machine learning and data science.
It can almost be guaranteed that the longer that you wait to recompile the image, the harder it will become.
If you are using an interactive container and find that you need to manually installed a package or change a parameter, it would be best practice to add this dependency to the container and rebuild it.
If the container is linked to an automated build provided by a version control service, simply pushing the `Dockerfile` to the service can easily trigger the build.
Docker images and layers can exhaust system resources, so it's good practice to regularly do `system prune --all` that will clean up stopped and unused containers, volumes, and networks. 
If you want to regularly prune unused container resources, you might even consider adding a cron job [REF] to do it.
After a prune is performed, it would follow naturally to rebuild a container for local usage, or to pull it again from a newly built registry image.
In the case of needing more setup or configuration than building the container, it's good practice to provide a `Makefile` alongside your container.
You generally should not rely on undocumented steps or manual, extra commands to be run on the local machine.
A fully scripted configuration makes it easier for both you and future users, and can increase trust in your workflow.
Finally, for a sanity check it can be helpful to ask a colleague to interact with your workflow or container.

<!-- I don't understand how these points fit in here-->
in your regular work, it is the only way to make sure it is really stable (cf. Marwick's "this container is the only way I have ever run this workflow")
- no showstopper for using UIs (web-based, e.g. Jupyter, RStudio, but also `x11docker`)
- **Don't replicate environment configuration outside of the Dockerfile for convenience**
  - avoid an untidy laboratory in practice behind a shiny appearance of a `Dockerfile`
  - you can install interactive UIs as part of the Dockerfile and use them just like Desktop UIs (Jupyter, RStudio, use )

<!-- I'm not sure how this fits in, should these tools be mentioned above when we talk about repo2docker?-->

# Example Dockerfiles

To demonstrate the 10 rules, we have a git repository with example `Dockerfile`s, some of which we took from public repositories and updated to adhere to the rules (`Dockerfile.before` and `Dockerfile.after`).

# Conclusion {#conclusion .unnumbered}

Reproducibility in research is an endeavour of best efforts, not about achieving the perfect solution, as that is probably not achievable or changing over time.
This article provides guidance for using `Dockerfile`s in computational/computer-based research to work towards a "time capsule" (see https://twitter.com/DougBlank/status/1135904909663068165?s=09) which, given some expertise and the right tools, can be used to come as close as possible to the original workflow.
Even if such a capsule decays over time, the effort to create it provides incredibly useful and valuable transparency for the project.
The effort should also be valued by others and may change the way scholars collaborate and communicate (cf. notion of "preproducibility" by @stark_before_2018)
So please, don't go insane with writing `Dockerfile`s, but be realistic about what might break and what is unlikely to break, and make a best effort.
In a similar vein, these rules may be broken if another way works better for _your use case_.
The rules in this article help you mastering the `Dockerfile` format for research and provide a solid basis for engaging in more complex but also in simpler assisted usage of containers (cf. Box: Aumatic Generation).
Corner cases aside, share and exchange your `Dockerfile` freely and collaborate in your community to spread the knowledge about containers as a tool for research.
Together you can develop common practices or even shared base images (exemplified by communities listed in Rule 1).

# Acknowledgements {#acknowledgements .unnumbered}

o2r by DFG

# Contributions {#contributions .unnumbered}

DN conceived the idea, wrote the first darft, contributed to all rules.
VS converted first draft bullet points into full sentences, contributed to all rules.
SE contributed to the overall structure and selected rules.
TH contributed to the rule structure and particularly rule 1.
THi gave extensive feedback on an early draft.

# References {#references .unnumbered}
