% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}




\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Writing Dockerfiles for Reproducible Research} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Daniel NÃ¼st\textsuperscript{\getAff{Institute for Geoinformatics, University of Muenster, Muenster, Germany}}\textsuperscript{*},
Vanessa Sochat\textsuperscript{\getAff{Stanford Research Computing Center, Stanford University, Stanford, CA,
US}},
Stephen Eglen\textsuperscript{\getAff{Department of Applied Mathematics and Theoretical Physics, University of
Cambridge, Cambridge, Cambridgeshire, GB}},
Tim Head\textsuperscript{\getAff{Wild Tree Tech, Zurich, CH}},
Tony Hirst\textsuperscript{\getAff{Department of Computing and Communications, The Open University, GB}}\\
\bigskip
\bigskip
* Corresponding author: daniel.nuest@uni-muenster.de\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Containers are greatly improving computational science by packaging
software and data dependencies. In a scholarly context, transparency and
support of reproducibility are the largest drivers for using these
containers. It follows that choices that are made with respect to
building containers can make or break a workflow's reproducibility. The
build for the container image is often created based on the instructions
in a plain-text file. For example, one such container technology,
Docker, provides instructions using a \texttt{Dockerfile}. By following
the rules in this article researchers writing a \texttt{Dockerfile} can
effectively build and distribute containers.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

With access to version control systems (VCS, {[}1{]}) and collaboration
platforms based on these, such as \href{https://github.com}{GitHub} or
\href{https://gitlab.com}{GitLab}, it has become increasingly easy to
not only share algorithms, but also instructions for executing research
workflows, including building and testing of the software used. The
publication of these instructions and related files (also known as
`artefacts') are a response to the increasing complexity of
computer-based research, which is too complicated for ``papers'' based
on the traditional journal article format to fully communicate the
details {[}2{]}, but where the actual contribution to knowledge is the
full software environment that produced a result {[}3{]}. A
\emph{research compendium} is a compilation of data, code, and
documentation that accompanies, or is itself, a scholarly publication
for increased transparency and reproducibility
(cf.~\url{https://research-compendium.science}). Within such a
compendium, it is desirable to include instructions, i.e.~a human- and
machine-readable recipe, for building containers that capture the
computing environment, i.e., all software and data dependencies. By
providing this recipe, authors of scientific articles greatly improve
their documentation and apply one important part of common practices for
scientific computing {[}4,5{]}, with the result that it is much more
likely both the author and others are able to reproduce and extend an
analysis workflow. The containers built from these recipes are portable
encapsulated snapshots of a specific computing environment. Such
containers have been demonstrated for capturing scientific notebooks
{[}6{]} and reproducible workflows (cf.~Rule 10 of {[}7{]}).

While there are several tutorials for using containers for reproducible
research {[}8--12{]}, there is no extensive examination for how to write
the actual instructions to create the containers. Several platforms for
facilitating reproducible research are built on top of containers
{[}13--17{]}, but they hide most of the complexity from the researcher.
Because \emph{``the number of unique research environments approximates
the number of researchers''} {[}17{]}, sticking to conventions helps
every researcher to understand, modify, and eventually write container
recipes, even if the are not sure how the technology behind them
actually works. Therefore researchers publishing a research compendium
should craft their own definition of computing environments following
good practices.

While there are many different container technologies, this article
focuses on Docker {[}18{]}. Docker is a highly suitable tool for
reproducible research (e.g., {[}19{]}) and our observations indicate it
is the most widely used container technology in academic data science.
The goal of this article is to guide you to write a \texttt{Dockerfile}
so that it best facilitates interactive development and computer-based
research, as well as the higher goals of reproducibility and
preservation of knowledge. Such practices are generally not part of
generic containerization tutorials and are rarely found in published
\texttt{Dockerfile}s, which are often used as templates by novices. The
differences between a helpful, stable \texttt{Dockerfile} and one that
is misleading, prone to failure, and full of potential obstacles, are
not obvious, especially for researchers who do not have software
development experience.

To start with, we assume you have a scripted scientific workflow,
i.e.~you can, at least at a certain point in time, execute the full
process with a single command, for example \texttt{make}. This execution
should be able to be triggered by command-line instruction, which is
possible for all generic programming languages and widely used workflow
tools, but may also be opening and starting a process with a graphical
user interface. A workflow that does not support scripted execution is
out of scope for reproducible research, and does not fit well with
containerization.

A commitment to these general practices can ensure that workflows are
reproducible, and generation of a container is not merely triggered by
the publication of a finished project (cf.~thoughts on openness as an
afterthought by {[}20{]} and on computational reproducibility by
{[}3{]}). By following the \emph{conventions} laid out in these ten
rules, authors ensure readability by others and enable subsequent reuse
and collaboration.

\hypertarget{docker}{%
\subsection*{Docker}\label{docker}}
\addcontentsline{toc}{subsection}{Docker}

Docker is a container technology that is widely adopted and supported on
many platforms, and has become highly useful in science. They are
distinct from virtual machines (VM) or hypervisors as they do not
emulate hardware, and thus do not require the same system resources. To
build Docker containers, we write text files that follow a particular
format called \texttt{Dockerfile}s {[}21{]}. \texttt{Dockerfile}s are
machine- and human-readable recipes for building images. Images are
inert, immutable, read-only files that includes the application
(e.g.~the programming language interpreter needed to run the workflow)
and the environment required by the application to run. They consist of
a sequence of instructions, which add layers to the image. These layers
can be used for caching across image builds, which is important for
minimizing build and download times. The images can then be run as
stateful containers, which are the running instances of Docker images.
Containers can be modified, stopped, restarted and purged.

While Docker was the original technology to support this format, other
container technologies have developed around the format (and thus
support it) including:
\href{https://podman.io/}{podman}/\href{https://github.com/containers/buildah}{buildah}
supported by RedHat,
\href{https://github.com/GoogleContainerTools/kaniko}{kaniko},
\href{https://github.com/genuinetools/img}{img}, and
\href{https://github.com/moby/buildkit}{buildkit}. The Singularity
container software {[}22{]} is optimized for high performance computing
and although it uses its own format, the \emph{Singularity recipe}, it
can import Docker containers directly from a Docker registry. Although
the Singularity recipe format is different, the rules here are
transferable to some extent. While some may argue for reasons to not
publish reproducibly (e.g.~lack of time and incentives, reluctance to
share, cf.~{[}23{]}) and there are substantial technical challenges to
maintain software and documentation, providing a \texttt{Dockerfile} or
pre-built Docker or other type of container should become an
increasingly easier task for the average researcher. If a researcher is
able to find or create containers or \texttt{Dockerfile}s to address
their most common use cases, then arguably it will not be extra work
after this initial set up (cf.~README of {[}24{]}). In fact, the
\texttt{Dockerfile} itself can be used as documentation to clearly show
from where data and code was derived, i.e.~downloaded or installed, and
consequently where a third party might obtain them again.

\hypertarget{consider-tools-to-assist-with-dockerfile-generation}{%
\section*{1. Consider tools to assist with Dockerfile
generation}\label{consider-tools-to-assist-with-dockerfile-generation}}
\addcontentsline{toc}{section}{1. Consider tools to assist with
Dockerfile generation}

Writing a \texttt{Dockerfile} from scratch is not that simple, and even
experts sometimes take shortcuts. Thus, it is a good strategy to first
look to tools that can help to generate a \texttt{Dockerfile} for you.
Such tools have likely thought about and implemented good practices, and
they may have added newer practices when reapplied at a later point in
time. It is always a good practice to start with your specific use case.
You first want to determine if there is an already existing container
that you can use, and in this case, use it and add to your workflow
documentation instructions for doing so. As an example, you might be
doing some kind of interactive development. For interactive development
environments such as notebooks and development servers or databases, you
can readily find containers that come installed with the software that
you need. In the case that there isn't an existing container for your
needs, you might next look to well-maintained tools to help with
\texttt{Dockerfile} generation. Well maintained means that recent
container bases are used, likely from the official Docker library
{[}25{]}, to ensure that the container has the most recent security
fixes for the operating system in question. As an example, repo2docker
{[}16{]} is a tool that is maintained by Jupyter Labs that can help to
transform a repository in the format of some known kind of package
(notebook, Python, R, etc.) into a container. Such a package contains
well-defined files for defining software dependencies and versions,
which repo2docker can understand. As an example, we might install
\texttt{jupyter-repo2docker} and then run it against a repository with a
\texttt{requirements.txt} file, an indication of being a Python package
with the following command.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{jupyter-repo2docker}\NormalTok{ https://github.com/norvig/pytudes}
\end{Highlighting}
\end{Shaded}

The resulting container image installs the dependencies listed in the
requirements file, along with providing an entrypoint to run a notebook
server to easily interact with any existing workflows in the repository.
A precaution that needs to be taken is that the default command above
will create a home for the current user, meaning that the container
itself would not be ideal to share, but rather any researchers
interested in interaction with the code inside should build their own
container. For this reason, it is good practice to look at any help
command provided by the tool and check for configuration options for
user names, user ids, and similar. It's also recommended (if you are
able) to add custom labels to your container build to define metadata
for your analyses (see Rule~6). The container should be built to be
optimized for its use case, whether it is intended to be shared or used
by a single user.

Additional tools to assist with writing Dockerfiles include
\texttt{containerit} {[}26{]} and \texttt{dockta} {[}27{]}.
\texttt{containerit} automates the generation of standalone
\texttt{Dockerfile}s for workflows in R. It can provide a starting point
for users unfamiliar with writing \texttt{Dockerfile}s, or together with
other R packages provide a full image creation and execution process
without having to leave an R session. \texttt{dockta} supports multiple
programming languages and configurations files, just as
\texttt{repo2docker}, but attempts to create readable
\texttt{Dockerfile}s compatible with plain Docker and to improve user
experience by cleverly adjusting instructions to reduce build time.

However, in the case that a tool or interactive container environment is
not available, or their capabilities to configure specific resources or
install bespoke tools are limited, you will likely need to write a
\texttt{Dockerfile} from scratch. In this case, proceed with following
the remaining rules to write your \texttt{Dockerfile}.

\hypertarget{use-versioned-and-automatically-built-base-images}{%
\section*{2. Use versioned and automatically built base
images}\label{use-versioned-and-automatically-built-base-images}}
\addcontentsline{toc}{section}{2. Use versioned and automatically built
base images}

It is good practice to use base images that are maintained by the Docker
library. While some organizations can be trusted to update containers
with security fixes (e.g., \texttt{rocker/r-ver}), for most individual
accounts that build containers, it is likely that containers will not be
updated regularly. It is even possible that images or
\texttt{Dockerfile}s disappear. Thus a good understanding of how base
images and image tags work is crucial, as the tag that you choose has
implications for your container build. You should know how to use
\texttt{FROM} statements to trace all the way up to the original
container base, an abstract base called \texttt{scratch}. Doing this
kind of trace is essential to be aware of all of the steps that were
taken to generate your container and who took them. If you want to build
on a third party image, carefully consider the author/maintainer and
\emph{never} use an image without a published \texttt{Dockerfile}. If
you want to use an unofficial image, do save a copy of the Dockerfile(s)
created by others. Alternatively, copy relevant instructions into your
own Dockerfile, acknowledge the source in a comment, and configure an
automated build for your own image. Automated builds can be complex to
set up, and the details are out of scope of this article. The
documentation and tools of container registries, e.g.,
\href{https://docs.docker.com/docker-hub/builds/}{Docker Hub} or
\href{https://docs.gitlab.com/ee/user/packages/container_registry/index.html\#build-and-push-images}{GitLab}
instances, but also of CI platforms,
e.g.~\href{https://github.com/actions/starter-workflows/tree/master/ci}{GitHub
actions}, a GitHub actions for \texttt{repo2docker} {[}28,29{]}, or
\href{https://circleci.com/orbs/registry/orb/circleci/docker\#commands-build}{CircleCI},
can help you get started. Using an automated build can help you avoid
publishing an image in a public registry with \texttt{docker\ push},
which is not ideal because using \texttt{docker\ push} in this way
opaquely breaks the linkage between a \texttt{Dockerfile} and its
resulting image.

A tag like \texttt{latest} is good in that security fixes and updates
are likely present, however it is bad in that it is a moving target so
that it is more likely that an updated base could break your workflow.
Other tags that should be avoided are \texttt{dev}, \texttt{devel}, or
\texttt{nightly} that may provide possibly unstable versions of the
software. As an example, the Python container on Docker Hub will be
released with new versions of Python. If you build a container with
\texttt{python:latest} that only supports Python version 3, when the
base is updated to Python 4 it's likely that your software won't work as
expected. In this case, a tag like \texttt{python:3.5} might be a good
choice, where the third component of the version, i.e., \texttt{3.5.x},
ensures that security patches and bugfixes that won't break your code,
i.e., changes that are backwards compatible, are applied (cf.~semantic
versioning, {[}30{]}). When you choose a base image, choose one with a
Linux distribution that supports the software stack you are using, and
also take into account the bases that are widely used by your community.
As an example, Ubuntu is heavily used for geospatial research, and so
the \texttt{rocker/geospatial} image would be a good choice for spatial
data science with R, or \texttt{jupyter/tensorflow-notebook} could be a
good choice for machine learning with Python. Especially when data is
involved, containers can grow quickly in size, and you should keep this
in mind. If you need to built smaller containers, consider a
\texttt{busybox} base or a multi-stage build, which allows to
selectively keep files from one build step to another. Smaller images
often are indicated by having \texttt{slim} or \texttt{minimal} as part
of the tag. Also take into account the libraries that you actually need.
Base images that have complex software installed (e.g.~machine learning
libraries, specific BLAS library) are helpful and fine to use. If you do
not want to rely on a third party or other individual to maintain the
recipe and container, you should copy the \texttt{Dockerfile} to your
own repository, and also provide a deployment for it via your own
automated build. Trusting that the third party \texttt{Dockerfile} will
persist for your analyses may be risky because that \texttt{Dockerfile}
could disappear without warning. Here is a selection of communities that
produce widely used regular builds and updates:

\begin{itemize}
\tightlist
\item
  \href{https://www.rocker-project.org/}{Rocker} for R {[}19{]}
\item
  \href{https://bioconductor.org/help/docker/}{Docker containers for
  Bioconductor} for bioinformatics
\item
  \href{https://hub.docker.com/_/neurodebian}{NeuroDebian images} for
  neuroscience {[}31{]}
\item
  {[}Jupyter Docker
  Stacks{]}(https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html
  for Notebook-based computing
\item
  \href{https://hub.docker.com/r/taverna/taverna-server}{Taverna Server}
  for running Taverna workflows
\end{itemize}

For example, here is how we would use a base image \texttt{r-ver} with
tag \texttt{3.5.2} from the \texttt{rocker} organization on Docker Hub
(\texttt{docker.io}).

\begin{verbatim}
FROM rocker/r-ver:3.5.2
\end{verbatim}

\hypertarget{use-formatting-document-within-and-favor-clarity}{%
\section{3. Use formatting, document within, and favor
clarity}\label{use-formatting-document-within-and-favor-clarity}}

It is good practice to think of the \texttt{Dockerfile} as a human
\emph{and} machine readable file. This means that you should use
indentation, new lines, and comments to make your \texttt{Dockerfile}s
well documented and easily readable. Specifically, carefully indent
commands and their arguments to make clear what belongs together,
especially when connecting multiple commands in a \texttt{RUN}
instruction with \texttt{\&\&}. Use \texttt{\textbackslash{}} at the end
of a line to break a single command into multiple lines. This will
ensure that no single line gets too long to comfortably read. Use long
versions of parameters for readability (e.g., \texttt{-\/-input} instead
of \texttt{-i}). When you need to change a directory, use
\texttt{WORKDIR}, because it not only creates the directory if it
doesn't exist but also persist the change across multiple \texttt{RUN}
instructions.

You can use a linter {[}32{]} to avoid small mistakes and follow good
practices from software development communities. The consistency added
by linting also helps keeping your edits to a \texttt{Dockerfile} in a
version control system (VCS) meaningful (see Rule~9). Note however that
a linter's rules may not primarily serve the intention of reproducible
scientific workflows.

As you are writing the \texttt{Dockerfile}, be mindful of how other
people will read it. Are your choices and commands being executed clear,
or is further comment warranted? To assist others in making sense of
your \texttt{Dockerfile}, you can add comments that include links to
online forums, code repository issues, or VCS commit messages to give
context for your specific decisions. Comments should include helpful
usage guides and links for readers inspecting the \texttt{Dockerfile},
including future you. Dependencies can be grouped in this fashion, which
also makes it easier to spot changes if your \texttt{Dockerfile} is
managed in a VCS (see Rule~9). It can even be helpful to include
comments about commands that did not work so you do not fall repeat past
mistakes. If you find that you need to remember an undocumented step,
that is an indication that it should be documented in the
\texttt{Dockerfile}.

Labels are useful to provide a more structured form of documentation
about software, especially for users of supporting infrastructure that
might not expose the actual \texttt{Dockerfile} - see Rule~6 for
details. It is often helpful to provide commented lines with
\texttt{docker\ build} and \texttt{docker\ run} within the
\texttt{Dockerfile} to show how to build and run the image, even though
these lines are not necessary for a valid \texttt{Dockerfile}, this is a
convenient place to record them. These comments can be especially
relevant if volume mounts or ports are important for using the
container, and by putting them at the end of the file, they are more
likely to be seen, easy to copy-paste after a container build, and to be
in consistent state compared to documentation in another file. Here is
an example of a commented section to show build and usage.

\begin{verbatim}
# Build the images with
##> docker build --tag great_workflow .
# Run the image:
##> docker run --it --port 80:80 --volume ./input:/input --name gwf great_workflow
# Extract the data:
##> docker cp gwf:/output/ ./output
\end{verbatim}

If you were to discover a previously written \texttt{Dockerfile} and not
remember the container identifier you used, it would be represented in
the \texttt{Dockerfile} in the \texttt{-\/-name} parameter, preserved in
a comment. Following a common coding aphorism, we might say \emph{``A
Dockerfile written three months ago may just as well have been written
by someone else''}. Here is a selection of typical kinds of comments
that are useful to include in a \texttt{Dockerfile}:

\begin{verbatim}
# apt-get install specific version, use 'apt-cache madison <pkg>' to see available versions
RUN apt-get install python3-pandas=0.23.3+dfsg-4ubuntu1

# RUN command spreading several lines
RUN R -e 'getOption("repos")' && \
  install2.r \
    fortunes \
    here

# this library must be installed from source to get version newer than in sources

# following commands from instructions at LINK HERE
\end{verbatim}

Clarity is always more important than brevity. For example, if your
container uses a script to run a complex install routine, instead of
removing it from the container upon completion, which is commonly seen
in production \texttt{Dockerfile}s aiming at small image size, you
should keep the script in the container for a future user to inspect.
Depending on the programming language used, your project may already
contain files to manage dependencies and you may use a package manager
to control this aspect of the computing environment. This is a very good
practice and helpful, though you should consider the externalization of
content to outside of the \texttt{Dockerfile} (see Rule~4). A single
long \texttt{Dockerfile} with sections and helpful comments can be
complete and thus more understandable than a collection of separate
files.

Generally, aim to design the \texttt{RUN} statements so that each
performs one scoped action (e.g., download, compile, and install one
tool). Each statement will result in a new layer, and reasonably grouped
changes increase readability of the \texttt{Dockerfile} and facilitate
inspection of the image, e.g.~with tools like dive {[}33{]}. A
\texttt{RUN} statement longer than a page requires scrolling,
diminishing readability. This may be challenging for the next reader to
digest and you should consider splitting it up, being aware of the extra
layers added to the image.

When you install several system libraries, it is good practice to add
comments about why the dependencies are needed. This way, if a piece of
software is removed from the container, it will be easier to remove the
system dependencies that are no longer needed. If you intend to build
the image more than once (perhaps during development) and you can take
advantage of build caching to avoid execution of time-consuming
instructions, e.g., install from a remote resource or a file that gets
cached. You should list commands \emph{in order} of least likely to
change to most likely to change and use the \texttt{-\/-no-cache} flag
to force a re-build of all layers. A recommended ordering based on this
metric might be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  system libraries
\item
  language-specific libraries or modules
\item
  from repositories (binaries)
\item
  from source
\item
  own software/scripts (if not mounted)
\item
  labels
\item
  \texttt{RUN}/\texttt{ENTRYPOINT}
\end{enumerate}

Finally, as a supplement to content inside the \texttt{Dockerfile}, it
is good practice to also write a section in a \texttt{README} alongside
the \texttt{Dockerfile} for exactly how to build, run, and otherwise
interact with the container. If a pre-built image is provided on Docker
Hub, you should direct the user to it in your \texttt{README}.

\hypertarget{pin-versions}{%
\section*{4. Pin versions}\label{pin-versions}}
\addcontentsline{toc}{section}{4. Pin versions}

The reproducibility of your \texttt{Dockerfile} is heavily dependent on
how well you define versions for dependencies inside.

\hypertarget{system-libraries}{%
\subsection{System libraries}\label{system-libraries}}

System library versions can largely come from the base image tag that
you choose to use (e.g., \texttt{ubuntu:18.04}) however, you can also
install specific versions of system packages with the respective package
manager. This practice is called version pinning (e.g., on apt:
https://blog.backslasher.net/my-pinning-guidelines.html). Version
pinning is good practice if the version is relevant. For example, you
might want to demonstrate a bug, prevent a bug in an updated version, or
pin a working version if you suspect an update could lead to a problem.
Generally, system libraries are more stable than software modules
supporting analysis scripts, but in some cases they can be highly
relevant to your workflow. \emph{Installing from source} is a useful way
to install very specific versions, however it comes at the cost of
needing build libraries. Here are some examples of terminal commands
that will list the currently installed versions of software on your
system:

\begin{itemize}
\tightlist
\item
  Debian/Ubuntu: \texttt{dpkg\ -\/-list}
\item
  Alpine: \texttt{apk\ -vv\ info\textbar{}sort}
\item
  CentOS: \texttt{yum\ list\ installed} or \texttt{rpm\ -qa}
\end{itemize}

\hypertarget{extension-packages-and-programming-language-modules}{%
\subsection{Extension packages and programming language
modules}\label{extension-packages-and-programming-language-modules}}

In the case of needing to install packages or dependencies for a
specific language, package managers are a good option. Package managers
generally provide reliable mirrors or endpoints to download software,
and many packages are tested before release. Most package managers have
a command line interface that can easily be used from \texttt{RUN}
commands in your \texttt{Dockerfile}, along with various flavors of
``freeze'' commands that can output a text file listing all software
packages and versions
(cf.~https://markwoodbridge.com/2017/03/05/jupyter-reproducible-science.html
cited by {[}6{]}) The biggest risk with using package managers with
respect to \texttt{Dockerfile}s is outsourcing configuration to file
formats that are not supported. As an example, here are configuration
files supported by commonly used languages in scientific programming:

\begin{itemize}
\tightlist
\item
  Python: \texttt{requirements.txt} (pip tool, {[}34{]}),
  \texttt{environment.yml} (Conda, {[}35{]})
\item
  R: \texttt{DESCRIPTION} file format {[}36{]} and \texttt{r}
  (``littler'', {[}37{]})
\item
  JavaScript: \texttt{package.json} of \texttt{npm} {[}38{]}
\item
  Julia: \texttt{Project.toml} and \texttt{Manifest.toml} {[}39{]}
\end{itemize}

In some cases (e.g., Conda) the package manager is also able to make
decisions about what versions to install, which is likely to lead to a
non-reproducible build. In all of the above, the user is required to
inspect the file or the build to see what is installed. For this reason,
in the case of having few packages, it is suggested to write the install
steps and versions directly into the \texttt{Dockerfile} (also for
clarity, see Rule 3). For example, the \texttt{RUN} statement here:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{RUN}\NormalTok{ pip install geopy==1.20.0 }\KeywordTok{&&} \KeywordTok{\textbackslash{}}
    \ExtensionTok{pip}\NormalTok{ install uszipcode==0.2.2}
\end{Highlighting}
\end{Shaded}

serves as more clear documentation in a \texttt{Dockerfile} than a
\texttt{requirements.txt} file that lists the same:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{RUN}\NormalTok{ pip install -r requirements.txt}
\end{Highlighting}
\end{Shaded}

This modularisation is a potential risk for understandability and
consistency (cf.~Rule 3), which can be mitigated by carefully organizing
all these files in the same version-controlled project. The version
pinning capabilities of these file formats are described in their
respective documentation.

\hypertarget{bind-mount-data-and-control-code}{%
\section*{5. Bind mount data and control
code}\label{bind-mount-data-and-control-code}}
\addcontentsline{toc}{section}{5. Bind mount data and control code}

Do not put data and workflow configuration or code only into the
container. It is better to \emph{bind}, or \emph{mount}, these files
from the host machine into the container, and using the container image
primarily for the software and dependencies. This is especially the case
if the container is intended to be used as an interactive environment.
This ensures that data persists when the container instance or image is
removed from your system. Other typical cases for binding or mounting is
when your data is very large (usually over hundreds of GB) or has access
restrictions. For example, a private dataset with personal information
(e.g.~health, financial, location, etc.) should never be added to an
image. In these cases, you should provide clear instructions to the user
in the \texttt{README} for how to obtain actual or dummy data. When
publishing your workspace, e.g.~on Zenodo, having the crucial content as
regular files outside of the container makes them more accessible to
others, for example for reuse or analysis.

You can use the \texttt{-v}/\texttt{-\/-volume} or \texttt{-\/-mount}
flags to \texttt{docker\ run} to configure bind mounts of directories or
files {[}40{]}, including options, as shown in the following examples.
If the target path exists in the image, the bind mount will replace it
for the specific container.

\begin{verbatim}
# mount directory
docker run --volume /home/user/project:/project mycontainer

# mount directory as read-only
docker run --volume /home/user/project:/project:ro mycontainer

# mount directory with write access relative to current path (Linux)
docker run --volume $(pwd)/inputdata:/data:rw mycontainer

# mount directory relative to current path (MacOS)
# mount directory relative to current path (Windows)
\end{verbatim}

Be careful about what volumes are bound to the host, because many
applications generate log and other output files during runtime. How you
bind external resources into the container should be included in the
example commands (see Rule 3). In these commands you can also make sure
to avoid issues with file permissions by using Docker's
\texttt{-\/-user} option. For example, by default, writing a new file
from inside the container will be owned by user \texttt{root} on your
host, because that is the default user within the container.

In specific cases is it feasible to add data or code into the container
using \texttt{ADD}/\texttt{COPY}. For example, to ensure you can
distribute a reproducible analysis it may be essential to add small data
files and software required for the container to function. If part of
the software developed for a specific analysis is published as a
conventional package you should follow Rule~4 for installing it from a
public repository, but you may also want to install it from source after
copying it into the image.

\hypertarget{capture-structured-environment-metadata}{%
\section*{6. Capture structured environment
metadata}\label{capture-structured-environment-metadata}}
\addcontentsline{toc}{section}{6. Capture structured environment
metadata}

Labels and build arguments can be very helpful to both provide metadata
and allow for customization of a build.

\hypertarget{labels}{%
\subsection{Labels}\label{labels}}

Labels serve as structured metadata that can be exposed by APIs, e.g.,
https://microbadger.com/labels, along with tools to inspect the
container binaries, e.g., \texttt{docker\ inspect}. For example,
software versions, maintainer contact information, along with vendor
specific metadata are commonly seen. The OCI Image Format Specification
provides some common label keys (see the ``Annotations'' section in
{[}41{]}) to help standardize field names across container tools, as
shown below.

\begin{verbatim}
LABEL org.opencontainers.image.created='2019-12-10' \
  org.opencontainers.image.authors='author@example.org' \
  org.opencontainers.image.url='https://github.com/nuest/ten-simple-rules' \
  org.opencontainers.image.documentation='https://github.com/nuest/ten-simple-rules/README.md' \
  org.opencontainers.image.version='0.0.1'

LABEL org.opencontainers.image.vendor='nuest' \
  org.opencontainers.image.title='Demo title' \
  org.opencontainers.image.description='Demo description'
\end{verbatim}

You can add multiple fields within the same instruction. Important
metadata attributes to include as labels would include any of the
following, ideally with globally unique identifiers:

\begin{itemize}
\tightlist
\item
  Author and contact (e.g.~email, project website; identified by a DOI;
  you will often see the deprecated \texttt{MAINTAINER} instruction -
  use a label instead
\item
  Research organizations (identified with https://ror.org/)
\item
  Funding agency/grant number
\item
  A repository link where the \texttt{Dockerfile} is published, e.g.~a
  GitHub project or a repository record with a DOI, e.g., Zenodo, where
  you can pre-register a DOI and add it to your \texttt{Dockerfile}
  before publishing the record
\item
  License
\end{itemize}

There are several projects (https://codemeta.github.io/,
https://citation-file-format.github.io/) focused on metadata citation,
and you should consider using one if it seems appropriate for your use
case.

\hypertarget{build-arguments}{%
\subsection{Build arguments}\label{build-arguments}}

Build arguments can provide more dynamic metadata and also allow for
customization of a build. As an example, the following build argument
would default to \texttt{1.0.0} but allow you to change it with
\texttt{-\/-build-arg\ MYVERSION=2.0.0} when building the image:

\begin{verbatim}
ARG MYVERSION=1.0.0
\end{verbatim}

Along with specifying versions, e.g., a git commit hash, or adding a
date and timestamp, build arguments can be useful to provide the context
of the build, e.g., building user, production versus development
environment, and automated or not. Examples of build arguments that are
useful to include to describe a container are:

\hypertarget{enable-interactive-usage-and-one-click-execution}{%
\section*{7. Enable interactive usage and one-click
execution}\label{enable-interactive-usage-and-one-click-execution}}
\addcontentsline{toc}{section}{7. Enable interactive usage and one-click
execution}

The success of a container can sometimes come down to how well it
exposes its usage and software to the user: a function that comes down
to the \texttt{CMD} and \texttt{ENTRYPOINT}. By using \texttt{CMD} and
\texttt{ENTRYPOINT}, you can use automated and manual testing to ensure
that it is possible to run the container interactively \emph{and} given
that execution is done incorrectly or has an error, a suitable help or
error message is shown to assist the user. A possible weakness with
using containers is the limitation on only providing one entrypoint,
however tools (e.g., The Scientific Filesystem, REF,
academic.oup.com/gigascience/article/7/5/giy023/4931737) have been
developed to expose multiple entrypoints, environments, help messages,
labels, and even install sequences. It is considered good practice to
have an entrypoint that meets reasonable user expectations. For example,
a container known to be a workflow should execute the workflow, or
provide instructions for how to do so. An interactive container should
spin up an analysis environment, ensuring that it prints to the screen
the ports that are being used (that need to be bound to the host) along
with any login credentials needed. Using a browser to expose a user
interface (e.g, RStudio, Jupyter, web tools) on a particular port is a
common practice, and for systems without native \texttt{x11} support,
\texttt{x11docker} is recommended {[}REF{]}. If your \texttt{Dockerfile}
does not give the user an intuitive interface immediately, you need to
clearly document how the user can start and control the graphical
interfaces provided inside. However, headless execution is often
desirable in that it can be tested without a graphical interface, either
by starting or interacting with the container and checking for
successful responses (e.g., 200) from endpoints provided, or by using a
controller such as Selenium {[}REF{]}. To support both ``one click
execution'' and to allow for custom configuration, it is helpful to
provide users with a configuration file, or allow for settings to be
expressed via environment variables {[}42{]}, or special Docker-based
wrappers such as Kliko {[}43{]}.

\hypertarget{establish-templates-for-new-projects}{%
\section*{8. Establish templates for new
projects}\label{establish-templates-for-new-projects}}
\addcontentsline{toc}{section}{8. Establish templates for new projects}

It is likely going to be the case that over time you will develop
workflows that are similar in nature to one another. In this case, you
should consider adopting a standard workflow that will give you a clean
slate for a new project. If you decide to build your own standard,
collaborate with your community during development of the standard to
ensure it will be useful to others. Part of your project template should
be a protocol for publishing the container image to a suitable container
registry, and taking into consideration of how the code can be given a
DOI or proper publication (e.g., Zenodo, Journal of Open Source
Software).

As an example, cookie cutter templates {[}44{]}, project starter kits
{[}REF{]}, or community templates (e.g.~{[}45{]}) can provide files,
directory organization, and build instructions that include basic steps
for getting started. A good project template should get you started with
a template for documentation, setting up testing via continuous
integration (CI), building a container, and even choosing a license. In
the case of using a common \texttt{Dockerfile} or base, Docker's build
caching will take shared lines into account and speed up build time even
between projects. When developing or working on projects with containers
you can easily switch between isolated project environments by stopping
the container and restarting it when you are ready to work again, even
on another machine or in a cloud environment. You can even run projects
in parallel without interference. At most, if a port is shared by two
projects to expose a user interface in the browser, you would need to
configure non-conflicting ports.

In the case of more complex web applications that require applications
and web servers, databases, and workers or messaging, the entire
infrastructure can easily be brought up or down with a combination of
templates and orchestration tools like \texttt{docker-compose} {[}46{]}.
\texttt{docker-compose} also allows definition of services using
multiple containers via it's own \texttt{docker-compose.yml} file. This
file can help to template options including mounted volumes, to
permissions, environment variable, and exposed ports.

\hypertarget{publish-one-dockerfile-per-project-in-a-code-repository-with-version-control}{%
\section*{9. Publish one Dockerfile per project in a code repository
with version
control}\label{publish-one-dockerfile-per-project-in-a-code-repository-with-version-control}}
\addcontentsline{toc}{section}{9. Publish one Dockerfile per project in
a code repository with version control}

Because a \texttt{Dockerfile} is a plain text-based format, it works
well with version control systems. Including a \texttt{Dockerfile}
alongside your code and (if size permits) data is an effective way to
consistently build your software, to show visitors to the repository how
it is built and used, to solicit feedback and collaborate with your
peers, and to increase the impact and sustainability of your work
(cf.~{[}47{]}). Online collaboration platforms (e.g., GitHub, GitLab)
also make it easy to use CI services to test building your image in an
independent build environment. Continuous integration increases
stability and trust, and gives the ability to publish images
automatically. If your \texttt{Dockerfile} includes an interactive user
interface, you can also adapt it so that it is ready-to-use as a Binder
instance {[}16{]}, providing an online work environment to any user with
a simple click of a link. Furthermore, the commit messages in your
version controlled repository preserve a record of all changes to the
\texttt{Dockerfile}.

While there are exceptions to the rule (cf.~{[}48{]}), it's generally a
simple and clear approach to provide one \texttt{Dockerfile} per
project. If you find that you need to provide more than one, use
\texttt{docker-compose} and consider if it's possible to use build
arguments to flip between states (e.g., development vs.~production, see
Rule~6) or to separate tools into different repositories.

\hypertarget{use-the-container-daily-rebuild-the-image-weekly-clean-up-and-preserve-if-need-be}{%
\section*{10. Use the container daily, rebuild the image weekly, clean
up and preserve if need
be}\label{use-the-container-daily-rebuild-the-image-weekly-clean-up-and-preserve-if-need-be}}
\addcontentsline{toc}{section}{10. Use the container daily, rebuild the
image weekly, clean up and preserve if need be}

Using containers for research workflows does not only require technical
understanding, but also an awareness of risks that can be managed
efficiently by following a number of good \emph{habits}, which we
outline below. While there is no firm rule, if you use a container
daily, is good practice to rebuild that container every once or two
weeks. At the time of publication of research results it is good
practice to save a copy of the image in a public data repository so that
readers of the publication can access the resources that produced the
published results.

First, use your container every time you work on a project and not just
as a final step during publication. If the container is the only
platform you use, the confidence in proper documentation of the
computing environment can be very high {[}49{]}. You should prioritize
this usage over others, e.g., non-interactive execution of a full
workflow, because it gives you personally the highest value and does not
limit your use or others' use of your data and code at all (see Rule~7).

Second, for reproducibility, we can treat containers as transient and
disposable, and even intentionally rebuild an image at regular
intervals. Ideally, containers that we built years ago should rebuild
seamlessly, but this is not necessarily the case, especially with
rapidly changing technology relevant to machine learning and data
science. It can almost be guaranteed that the longer that you wait to
rebuild the image, the more likely you are to encounter an error that
will interfere with the build process. If you are using an interactive
container and find that you need to manually install a package or change
a parameter, it is best practice to add this dependency to the container
and rebuild it right away, but one tends to take shortcuts. Therefore, a
habitual deletion of a container and cache-less rebuild of the image not
only increases security due to updating underlying software, but also
helps to reveals issues requiring manual interference, i.e., changes to
code or configuration not documented in the \texttt{Dockerfile} (but
perhaps should be). This habit can be easily supported by using
continuous deployment or CI strategies. If the container is linked to an
automated build, then pushing updates to a VCS repository with a
\texttt{Dockerfile} can easily trigger a build to validate execution of
the \texttt{Dockerfile}.

In the case of needing setup or configuration for the first two habits,
it is good practice to provide a \texttt{Makefile} alongside your
container, which can capture the specific commands. The effective use of
a \texttt{Makefile} can help avoiding undocumented steps or manual,
extra commands to be run on the local machine. A fully scripted
configuration makes it easier for both you and future users, and can
increase trust in your workflow.

Third, from time to time you can reduce the system resources occupied by
Docker images and their layers or unused containers, volumes and
networks by running \texttt{docker\ system\ prune\ -\/-all}. After a
prune is performed, it follows naturally to rebuild a container for
local usage, or to pull it again from a newly built registry image. This
habit can be automated with a cron job {[}50{]}.

Fourth, you can export the image to file and deposit it in a public data
repository, where it not only becomes citable but also provides a
snapshot of the \emph{actual} environment you used at a specific point
in time. You should include instructions how to import and run the
workflow based on the image archive. Depositing the image with other
project files (e.g.~data, code, \texttt{Dockerfile}) in a public
repository makes them likely to be preserved. Applying proper
preservation strategies (cf.~{[}47{]}) can be highly complex, but simply
running an image ``as-is'', i.e.~with the default command and entrypoint
(see Rule~7), and observing the output is quite likely to work for many
years into the future. If the image does not work anymore, a user can
still extract the image contents and explore the files of each layer
manually, or if an import still works, with exploration tools like dive
{[}33{]}. However, if you want to ensure usability and extendability,
then you could run import, run, and export an image regularly to make
sure the export format still works with the then current version of
Docker. The exported image and a version controlled \texttt{Dockerfile}
together allow you to freely experiment and continue development of your
workflow and keeping the image up to date, e.g., updating versions of
pinned dependencies (see Rule~4) and regular image building (see above).

Finally, for a sanity check and to foster even higher trust in the
stability and documentation of your project, you can ask a colleague or
community member to be your code copilot (see
\url{https://twitter.com/Code_Copilot}) to interact with your workflow
container on a machine of their own. You can do this shortly before
submitting your reproducible workflow for peer-review, so you are well
positioned for the future of scholarly communication and open science
where these may be standard practices required for publication
{[}20,51--53{]}.

\hypertarget{example-dockerfiles}{%
\section{Example Dockerfiles}\label{example-dockerfiles}}

To demonstrate the ten rules, we maintain a GitHub repository with
example \texttt{Dockerfile}s, some of which we took from public
repositories and updated to adhere to the rules (see
\texttt{Dockerfile.original}):
\url{https://github.com/nuest/ten-simple-rules-dockerfiles/}

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

Reproducibility in research is an endeavor of incremental improvement
and best efforts, not about achieving the perfect solution, which may be
not achievable for many researchers with limited resources, and the
definition of which may change over time. In this article we have
provided guidance for using \texttt{Dockerfile}s in computational
research. Our goal is to help the researcher to work towards creating a
``time capsule'' {[}54{]} which, given some expertise and the right
tools, can be used to come as close as possible to the original workflow
with reasonably little effort. Even if such a capsule decays over time,
the effort to create and document it provides incredibly useful and
valuable transparency for the project. We encourage researchers to value
these steps taken by their peers to use \texttt{Dockerfile}s to create
``time capsules'', and promote change in the way scholars communicate
(cf.~{[}55{]}'s notion of ``preproducibility'' ). So please, make a best
effort with your current knowledge, and strive to write efficient,
readable \texttt{Dockerfile}s that are realistic about what might break
and what is unlikely to break. In a similar vein, we accept that you
should freely break these rules if another way makes more sense
\emph{for your use case}. Most importantly, share and exchange your
\texttt{Dockerfile} freely and collaborate in your community to spread
the knowledge about containers as a tool for research and scholarly
collaboration and communication. Together we can develop common
practices and shared materials for better transparency, higher
efficiency, and faster innovation.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

DN is supported by the project Opening Reproducible Research II
(\href{https://o2r.info/}{https://o2r.info/};
\href{https://www.uni-muenster.de/forschungaz/project/12343}{https://www.uni-muenster.de/forschungaz/project/12343})
funded by the German Research Foundation (DFG) under project number PE
1632/17-1.

\hypertarget{contributions}{%
\section*{Contributions}\label{contributions}}
\addcontentsline{toc}{section}{Contributions}

DN conceived the idea, wrote the first draft, and contributed to all
rules. VS wrote the first draft based on notes and contributed to all
rules. SE contributed to the overall structure and selected rules. THe
contributed to the rule structure and particularly rule~1. THi gave
extensive feedback on early drafts and contributed to the discussion.
This articles was written collaboratively on GitHub, where all
contributions in form of text or discussions comments are documented:
\url{https://github.com/nuest/ten-simple-rules-dockerfiles/}.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-wikipedia_contributors_version_2019}{}%
1. Wikipedia contributors. Version control {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Version_control\&oldid=926593231}

\leavevmode\hypertarget{ref-marwick_how_2015}{}%
2. Marwick B. How computers broke science -- and what we can do to fix
it {[}Internet{]}. The Conversation. 2015. Available:
\url{https://theconversation.com/how-computers-broke-science-and-what-we-can-do-to-fix-it-49938}

\leavevmode\hypertarget{ref-donoho_invitation_2010}{}%
3. Donoho DL. An invitation to reproducible computational research.
Biostatistics. 2010;11: 385--388.
doi:\href{https://doi.org/10.1093/biostatistics/kxq028}{10.1093/biostatistics/kxq028}

\leavevmode\hypertarget{ref-wilson_best_2014}{}%
4. Wilson G, Aruliah DA, Brown CT, Hong NPC, Davis M, Guy RT, et al.
Best Practices for Scientific Computing. PLOS Biology. 2014;12:
e1001745.
doi:\href{https://doi.org/10.1371/journal.pbio.1001745}{10.1371/journal.pbio.1001745}

\leavevmode\hypertarget{ref-wilson_good_2017}{}%
5. Wilson G, Bryan J, Cranston K, Kitzes J, Nederbragt L, Teal TK. Good
enough practices in scientific computing. PLOS Computational Biology.
2017;13: e1005510.
doi:\href{https://doi.org/10.1371/journal.pcbi.1005510}{10.1371/journal.pcbi.1005510}

\leavevmode\hypertarget{ref-rule_ten_2019}{}%
6. Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et
al. Ten simple rules for writing and sharing computational analyses in
Jupyter Notebooks. PLOS Computational Biology. 2019;15: e1007007.
doi:\href{https://doi.org/10.1371/journal.pcbi.1007007}{10.1371/journal.pcbi.1007007}

\leavevmode\hypertarget{ref-sandve_ten_2013}{}%
7. Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for
Reproducible Computational Research. PLoS Comput Biol. 2013;9: e1003285.
doi:\href{https://doi.org/10.1371/journal.pcbi.1003285}{10.1371/journal.pcbi.1003285}

\leavevmode\hypertarget{ref-nust_author_2017}{}%
8. NÃ¼st D. Author Carpentry : Docker for reproducible research
{[}Internet{]}. Author Carpentry : Docker for reproducible research.
2017. Available:
\url{https://nuest.github.io/docker-reproducible-research/}

\leavevmode\hypertarget{ref-chapman_reproducible_2018}{}%
9. Chapman P. Reproducible data science environments with Docker Phil
Chapman's Blog {[}Internet{]}. 2018. Available:
\url{https://chapmandu2.github.io/post/2018/05/26/reproducible-data-science-environments-with-docker/}

\leavevmode\hypertarget{ref-ropensci_labs_r_2015}{}%
10. rOpenSci Labs. R Docker tutorial {[}Internet{]}. 2015. Available:
\url{https://ropenscilabs.github.io/r-docker-tutorial/}

\leavevmode\hypertarget{ref-udemy_docker_2019}{}%
11. Udemy, Zhbanko V. Docker Containers for Data Science and
Reproducible Research {[}Internet{]}. Udemy. 2019. Available:
\url{https://www.udemy.com/course/docker-containers-data-science-reproducible-research/}

\leavevmode\hypertarget{ref-psomopoulos_lesson_2017}{}%
12. Psomopoulos FE. Lesson "Docker and Reproducibility" in Workshop
"Reproducible analysis and Research Transparency" {[}Internet{]}.
Reproducible analysis and Research Transparency. 2017. Available:
\url{https://reproducible-analysis-workshop.readthedocs.io/en/latest/8.Intro-Docker.html}

\leavevmode\hypertarget{ref-brinckman_computing_2018}{}%
13. Brinckman A, Chard K, Gaffney N, Hategan M, Jones MB, Kowalik K, et
al. Computing environments for reproducibility: Capturing the ``Whole
Tale''. Future Generation Computer Systems. 2018;
doi:\href{https://doi.org/10.1016/j.future.2017.12.029}{10.1016/j.future.2017.12.029}

\leavevmode\hypertarget{ref-code_ocean_2019}{}%
14. Code Ocean {[}Internet{]}. 2019. Available:
\url{https://codeocean.com/}

\leavevmode\hypertarget{ref-simko_reana_2019}{}%
15. Å imko T, Heinrich L, Hirvonsalo H, Kousidis D, RodrÃ­guez D. REANA: A
System for Reusable Research Data Analyses. EPJ Web of Conferences.
2019;214: 06034.
doi:\href{https://doi.org/10.1051/epjconf/201921406034}{10.1051/epjconf/201921406034}

\leavevmode\hypertarget{ref-jupyter_binder_2018}{}%
16. Jupyter P, Bussonnier M, Forde J, Freeman J, Granger B, Head T, et
al. Binder 2.0 - Reproducible, interactive, sharable environments for
science at scale. Proceedings of the 17th Python in Science Conference.
2018; 113--120.
doi:\href{https://doi.org/10.25080/Majora-4af1f417-011}{10.25080/Majora-4af1f417-011}

\leavevmode\hypertarget{ref-nust_opening_2017}{}%
17. NÃ¼st D, Konkol M, Pebesma E, Kray C, Schutzeichel M, Przibytzin H,
et al. Opening the Publication Process with Executable Research
Compendia. D-Lib Magazine. 2017;23.
doi:\href{https://doi.org/10.1045/january2017-nuest}{10.1045/january2017-nuest}

\leavevmode\hypertarget{ref-wikipedia_contributors_docker_2019}{}%
18. Wikipedia contributors. Docker (software) {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Docker_(software)\&oldid=928441083}

\leavevmode\hypertarget{ref-boettiger_introduction_2017}{}%
19. Boettiger C, Eddelbuettel D. An Introduction to Rocker: Docker
Containers for R. The R Journal. 2017;9: 527--536.
doi:\href{https://doi.org/10.32614/RJ-2017-065}{10.32614/RJ-2017-065}

\leavevmode\hypertarget{ref-chen_open_2019}{}%
20. Chen X, Dallmeier-Tiessen S, Dasler R, Feger S, Fokianos P, Gonzalez
JB, et al. Open is not enough. Nature Physics. 2019;15: 113.
doi:\href{https://doi.org/10.1038/s41567-018-0342-2}{10.1038/s41567-018-0342-2}

\leavevmode\hypertarget{ref-docker_inc_dockerfile_2019}{}%
21. Docker Inc. Dockerfile reference {[}Internet{]}. Docker
Documentation. 2019. Available:
\url{https://docs.docker.com/engine/reference/builder/}

\leavevmode\hypertarget{ref-kurtzer_singularity_2017}{}%
22. Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers
for mobility of compute. PLOS ONE. 2017;12: e0177459.
doi:\href{https://doi.org/10.1371/journal.pone.0177459}{10.1371/journal.pone.0177459}

\leavevmode\hypertarget{ref-boettiger_introduction_2015}{}%
23. Boettiger C. An Introduction to Docker for Reproducible Research.
SIGOPS Oper Syst Rev. 2015;49: 71--79.
doi:\href{https://doi.org/10.1145/2723872.2723882}{10.1145/2723872.2723882}

\leavevmode\hypertarget{ref-marwick_madjebebe_2015}{}%
24. Ben Marwick. 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-docker_inc_official_2019}{}%
25. Docker Inc. Official Images on Docker Hub {[}Internet{]}. Docker
Documentation. 2019. Available:
\url{https://docs.docker.com/docker-hub/official_images/}

\leavevmode\hypertarget{ref-nust_containerit_2019}{}%
26. NÃ¼st D, Hinz M. Containerit: Generating Dockerfiles for reproducible
research with R. Journal of Open Source Software. 2019;4: 1603.
doi:\href{https://doi.org/10.21105/joss.01603}{10.21105/joss.01603}

\leavevmode\hypertarget{ref-stencila_dockta_2019}{}%
27. Stencila. Stencila/dockta {[}Internet{]}. Stencila; 2019. Available:
\url{https://github.com/stencila/dockta}

\leavevmode\hypertarget{ref-husain_repo2docker-action_2019}{}%
28. Husain H, Silkaitis R. Machine-learning-apps/repo2docker-action
{[}Internet{]}. ML Apps; 2019. Available:
\url{https://github.com/machine-learning-apps/repo2docker-action}

\leavevmode\hypertarget{ref-scottyhq_repo2docker-githubci_2019}{}%
29. Henderson S. Scottyhq/repo2docker-githubci {[}Internet{]}. 2019.
Available: \url{https://github.com/scottyhq/repo2docker-githubci}

\leavevmode\hypertarget{ref-preston-werner_semantic_2013}{}%
30. Preston-Werner T. Semantic Versioning 2.0.0 {[}Internet{]}. Semantic
Versioning. 2013. Available: \url{https://semver.org/}

\leavevmode\hypertarget{ref-halchenko_open_2012}{}%
31. Halchenko YO, Hanke M. Open is Not Enough. Let's Take the Next Step:
An Integrated, Community-Driven Computing Platform for Neuroscience.
Frontiers in Neuroinformatics. 2012;6.
doi:\href{https://doi.org/10.3389/fninf.2012.00022}{10.3389/fninf.2012.00022}

\leavevmode\hypertarget{ref-wikipedia_contributors_lint_2019}{}%
32. Wikipedia contributors. Lint (software) {[}Internet{]}. Wikipedia.
2019. Available:
\url{https://en.wikipedia.org/w/index.php?title=Lint_(software)\&oldid=907589761}

\leavevmode\hypertarget{ref-goodman_dive_2019}{}%
33. Goodman A. Wagoodman/dive {[}Internet{]}. 2019. Available:
\url{https://github.com/wagoodman/dive}

\leavevmode\hypertarget{ref-the_python_software_foundation_requirements_2019}{}%
34. The Python Software Foundation. Requirements Files --- pip User
Guide {[}Internet{]}. 2019. Available:
\url{https://pip.pypa.io/en/stable/user_guide/\#requirements-files}

\leavevmode\hypertarget{ref-continuum_analytics_managing_2017}{}%
35. Continuum Analytics. Managing environments --- conda documentation
{[}Internet{]}. 2017. Available:
\url{https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html}

\leavevmode\hypertarget{ref-r_core_team_description_1999}{}%
36. R Core Team. The DESCRIPTION file in "writing r extensions"
{[}Internet{]}. 1999. Available:
\url{https://cran.r-project.org/doc/manuals/r-release/R-exts.html\#The-DESCRIPTION-file}

\leavevmode\hypertarget{ref-eddelbuettel_littler_2019}{}%
37. Eddelbuettel D, Horner J. Littler: R at the command-line via 'r'
{[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=littler}

\leavevmode\hypertarget{ref-npm_creating_2019}{}%
38. npm. Creating a package.json file npm Documentation {[}Internet{]}.
2019. Available:
\url{https://docs.npmjs.com/creating-a-package-json-file}

\leavevmode\hypertarget{ref-julia_tomls_2019}{}%
39. The Julia Language Contributors. 10. Project.Toml and Manifest.Toml
Â· Pkg.Jl {[}Internet{]}. 2019. Available:
\url{https://julialang.github.io/Pkg.jl/v1/toml-files/}

\leavevmode\hypertarget{ref-docker_use_2019}{}%
40. Docker Inc. Use bind mounts {[}Internet{]}. Docker Documentation.
2019. Available: \url{https://docs.docker.com/storage/bind-mounts/}

\leavevmode\hypertarget{ref-opencontainers_image-spec_2017}{}%
41. Opencontainers. Opencontainers/image-spec v1.0.1 - Annotations
{[}Internet{]}. GitHub. 2017. Available:
\url{https://github.com/opencontainers/image-spec/blob/v1.0.1/annotations.md}

\leavevmode\hypertarget{ref-knoth_reproducibility_2017}{}%
42. Knoth C, NÃ¼st D. Reproducibility and Practical Adoption of GEOBIA
with Open-Source Software in Docker Containers. Remote Sensing. 2017;9:
290. doi:\href{https://doi.org/10.3390/rs9030290}{10.3390/rs9030290}

\leavevmode\hypertarget{ref-molenaar_klikoscientific_2018}{}%
43. Molenaar G, Makhathini S, Girard JN, Smirnov O. Kliko---The
scientific compute container format. Astronomy and Computing. 2018;25:
1--9.
doi:\href{https://doi.org/10.1016/j.ascom.2018.08.003}{10.1016/j.ascom.2018.08.003}

\leavevmode\hypertarget{ref-cookiecutter_contributors_cookiecutter_2019}{}%
44. \{Cookiecutter contributors\}. Cookiecutter/cookiecutter
{[}Internet{]}. cookiecutter; 2019. Available:
\url{https://github.com/cookiecutter/cookiecutter}

\leavevmode\hypertarget{ref-marwick_rrtools_2019}{}%
45. Marwick B. Benmarwick/rrtools {[}Internet{]}. 2019. Available:
\url{https://github.com/benmarwick/rrtools}

\leavevmode\hypertarget{ref-docker-compose_2019}{}%
46. Docker Inc. Overview of Docker Compose {[}Internet{]}. Docker
Documentation. 2019. Available: \url{https://docs.docker.com/compose/}

\leavevmode\hypertarget{ref-emsley_framework_2018}{}%
47. Emsley I, De Roure D. A Framework for the Preservation of a Docker
Container International Journal of Digital Curation. International
Journal of Digital Curation. 2018;12.
doi:\href{https://doi.org/10.2218/ijdc.v12i2.509}{10.2218/ijdc.v12i2.509}

\leavevmode\hypertarget{ref-kim_bio-docklets_2017}{}%
48. Kim B, Ali TA, Lijeron C, Afgan E, Krampis K. Bio-Docklets:
Virtualization Containers for Single-Step Execution of NGS Pipelines.
bioRxiv. 2017; 116962.
doi:\href{https://doi.org/10.1101/116962}{10.1101/116962}

\leavevmode\hypertarget{ref-marwick_readme_2015}{}%
49. Marwick B. README of 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-wikipedia_contributors_cron_2019}{}%
50. Wikipedia contributors. Cron {[}Internet{]}. Wikipedia. 2019.
Available:
\url{https://en.wikipedia.org/w/index.php?title=Cron\&oldid=929379536}

\leavevmode\hypertarget{ref-eglen_codecheck_2019}{}%
51. Eglen S, NÃ¼st D. CODECHECK: An open-science initiative to facilitate
sharing of computer programs and results presented in scientific
publications. Septentrio Conference Series. 2019;
doi:\href{https://doi.org/10.7557/5.4910}{10.7557/5.4910}

\leavevmode\hypertarget{ref-schonbrodt_training_2019}{}%
52. SchÃ¶nbrodt F. Training students for the Open Science future. Nature
Human Behaviour. 2019;3: 1031--1031.
doi:\href{https://doi.org/10.1038/s41562-019-0726-z}{10.1038/s41562-019-0726-z}

\leavevmode\hypertarget{ref-eglen_recent_2018}{}%
53. Eglen SJ, Mounce R, Gatto L, Currie AM, Nobis Y. Recent developments
in scholarly publishing to improve research practices in the life
sciences. Emerging Topics in Life Sciences. 2018;2: 775--778.
doi:\href{https://doi.org/10.1042/ETLS20180172}{10.1042/ETLS20180172}

\leavevmode\hypertarget{ref-blank_twitter_2019}{}%
54. Blank D, Twitter. Twitter thread on reproducibility time capsules on
Twitter {[}Internet{]}. Twitter. 2019. Available:
\url{https://twitter.com/dougblank/status/1135904909663068165}

\leavevmode\hypertarget{ref-stark_before_2018}{}%
55. Stark PB. Before reproducibility must come preproducibility
{[}Internet{]}. Nature. 2018.
doi:\href{https://doi.org/10.1038/d41586-018-05256-0}{10.1038/d41586-018-05256-0}

\nolinenumbers


\end{document}

