% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}





\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Writing Dockerfiles for Reproducible Research} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Daniel NÃ¼st\textsuperscript{\getAff{Institute for Geoinformatics, University of M``unster, M''unster,
Germany}}\textsuperscript{*},
Vanessa Sochat\textsuperscript{\getAff{Stanford Research Comuting Center, Stanford University, Stanford, CA, US}},
Stephen Eglen\textsuperscript{\getAff{Department of Applied Mathematics and Theoretical Physics, University of
Cambridge, Cambridge, Cambridgeshire, GB}},
Tim Head\textsuperscript{\getAff{Wild Tree Tech, Zurich, CH}}\\
\bigskip
\bigskip
* Corresponding author: daniel.nuest@uni-muenster.de\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Containers are greatly improving computational science by packaging
software and data dependencies. In a scholarly context, transparency and
support of reproducibility are the largest drivers for using these
containers. It follows that choices that are made with respect to
building containers can make or break its reproducibility. The build for
the container image is often created based on the instructions in a
plain-text file. For example, one such container technology, Docker,
provides instructions using a \texttt{Dockerfile}. By following the
rules in this article researchers writing a \texttt{Dockerfile} can
effectively build and distribute containers.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.
\section*{Author summary}
TBD

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

With access to version control systems {[}REF GITHUB, GITLAB, etc.{]} it
has become increasingly easy to not only share algorithms, but also
instructions for building and testing. Within this content, it is
suggested to include instructions for building containers to package
software and data dependencies {[}1{]}. By way of providing these
instructions along with thorough documentation, it's much more likely to
be able to reproduce an analysis workflow. A container is a portabal,
encapsulated environment that is built from human and machine readable
instructions. For example, containers have been created to host
scientific notebooks {[}2{]}, and share reproducible workflows (cf.~Rule
10 of {[}3{]}).

While there are several tutorials for using containers for reproducible
research,\footnote{https://nuest.github.io/docker-reproducible-research/,
  https://chapmandu2.github.io/post/2018/05/26/reproducible-data-science-environments-with-docker/,
  https://reproducible-analysis-workshop.readthedocs.io/en/latest/8.Intro-Docker.html}
there is no extensive examination for how to write the actual
instructions to create the containers. Several platforms for
facilitating reproducible research are built on top of containers
{[}4--8{]}, but they hide any complexity from the researcher. While not
everybody needs to understand and write Dockerfiles, as \emph{``the
number of unique research environments approximates the number of
researchers''} {[}8{]}, the researchers who do need to craft their own
environment definition should follow good practices. Such practices are
not part of generic Docker tutorials and are not present in existing
Dockerfiles often used as templates. The differences and potential
obstacles are not obvious, especially for researchers who don't have
software development experience.

While there are many different container technologies, this article
focuses on Docker {[}REF{]} as it is the most commonly used {[}REF{]}.
The goal at hand is to write a \texttt{Dockerfile} so that it best
facilitates interactive development and research as well as the higher
goals of reproducibility. A daily commitment to these general practices
can ensure that workflows are reproducible, and generation of a
container is not an afterthought triggered by a publication
(cf.~thoughts on openness as an afterthought by {[}9{]} and on
computational reproducibility by {[}10{]}). By following the
\emph{conventions} layed out in these ten rules, authors ensure
readability by others and ideally subsequent reuse and collaboration.

\hypertarget{docker}{%
\subsection*{Docker}\label{docker}}
\addcontentsline{toc}{subsection}{Docker}

\begin{itemize}
\tightlist
\item
  Docker is a common containerisation solution, widely adopted in
  mainstream IT and therefore widely available and support on many
  platforms, which makes it usable for non-IT experts in science
\item
  \texttt{Dockerfile}s are machine- and human-readable recipes for
  creating a container
\item
  Other containerisation tools also support the \texttt{Dockerfile}
  format:
  \href{https://podman.io/}{podman}/\href{https://github.com/containers/buildah}{buildah},
  \href{https://github.com/GoogleContainerTools/kaniko}{kaniko},
  \href{https://github.com/genuinetools/img}{img}, or
  \href{https://github.com/moby/buildkit}{buildkit}; Singularity
  {[}11{]} can import Docker containers and has it's own ``singularity
  recipee'', to which the rules here are transferable
\item
  {[}12{]} lists reasons for not publishing reproducibly include lack of
  time \& incentives and unfittingness for a researcher's workflow, and
  gives technical challenges of virtual laboratories, namely dependency
  hell, imprecise documentation, code rot, and handling/learning
  multiple tools

  \begin{itemize}
  \tightlist
  \item
    learn to write a \texttt{Dockerfile} with this article and use it as
    the \emph{only} environment that you execute your workflows in
    (cf.~README of {[}13{]}), then it will not be extra work
    \textgreater{} \emph{``I developed and tested the package on this
    Docker container, so this is the only platform that I'm confident it
    works on, and so recommend to anyone wanting to use this package to
    generate the vignette, etc.''}
  \item
    VMs are not an alternative because too much of a black box
  \end{itemize}
\item
  In RR, Dockerfiles can document where data and code came from and
  likely also where a third party might still get them
\end{itemize}

\hypertarget{dont-write-dockerfiles-by-hand}{%
\section*{1. Don't write Dockerfiles by
hand}\label{dont-write-dockerfiles-by-hand}}
\addcontentsline{toc}{section}{1. Don't write Dockerfiles by hand}

\begin{itemize}
\tightlist
\item
  might seem counterintuitive rule
\item
  many cases you don't need a Dockerfile but just need a container with
  an environment to run your workflow
\item
  use a tool to generate container (via a Dockerfile) for you

  \begin{itemize}
  \tightlist
  \item
    \texttt{repo2docker} which can automatically create a \emph{Binder}
    for you; you don't worry about the internal Dockerfile but get the
    container by following common practices for software packages
  \item
    tools follow the relevant rules below
  \end{itemize}
\item
  writing a Dockerfile from scratch is not that simple, and even
  ``experts'' sometimes take shortcuts
\item
  if you're sure your needs are not served, then continue with other
  rules
\end{itemize}

\hypertarget{use-versioned-and-automatically-built-base-images}{%
\section*{2. Use versioned and automatically built base
images}\label{use-versioned-and-automatically-built-base-images}}
\addcontentsline{toc}{section}{2. Use versioned and automatically built
base images}

\begin{itemize}
\tightlist
\item
  understand how base images work
\item
  \emph{never} use \texttt{:latest}
\item
  pick a base image with a Linux distribution that supports the required
  software stack, and ideally that is widely used in your community,
  e.g.~Ubuntu for geospatial research, the \texttt{rocker/geospatial}
  image for spatial data science with R, or
  \texttt{jupyter/tensorflow-notebook} for machine learning with Python
\item
  base images (all the way to the top) must be based on Dockerfiles
  themselves
\item
  library base images are well maintained and security tested, but
  alternatives might be more suitable for research purposes / RR
  (example \texttt{rocker/r-ver})
\item
  base images that have complex software installed (e.g.~ML libraries,
  specific BLAS library) are helpful and fine to use, just ensure there
  is a Dockerfile publicly available that they use (and add a link to
  that file in your Dockerfile)
\item
  ideally the images are maintained by an active community/\emph{your
  community}
\item
  existing communities

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.rocker-project.org/}{Rocker} for R {[}1{]}
  \item
    \href{https://bioconductor.org/help/docker/}{Docker containers for
    Bioconductor} for bioinformatics
  \item
    \href{https://hub.docker.com/_/neurodebian}{NeuroDebian images} for
    neuroscience {[}14{]}
  \item
    {[}Jupyter Docker
    Stacks{]}(https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html
    for Notebook-based computing
  \item
    \href{https://hub.docker.com/r/taverna/taverna-server}{Taverna
    Server} for running Taverna workflows
  \end{itemize}
\end{itemize}

\begin{verbatim}
FROM docker.io/rocker/r-ver:3.5.2
\end{verbatim}

\hypertarget{use-formatting-and-favour-clarity}{%
\section{3. Use formatting and favour
clarity}\label{use-formatting-and-favour-clarity}}

\begin{itemize}
\tightlist
\item
  indentation, newlines, and comments for are crucial for documentation,
  readability and structure
\item
  use comments to provide a guide to others and future you, \emph{``A
  Dockerfile written three months ago may just as well have been written
  by someone else''} (following a common coding aphorism)
\item
  document for your future self first but tend to overexplain (cf.~Rule
  1 of {[}2{]}), provide extended docs if others ask for it
\item
  can use comments to add sections to the Dockerfile to reduce the need
  to externalise when files get long

  \begin{itemize}
  \tightlist
  \item
    modularisation is a two-edged sword (point out
    \href{https://www.mankier.com/1/podman-build}{\texttt{podman}'s
    \texttt{\#include} directive}?)
  \end{itemize}
\item
  carefully indent commands and their arguments to make clear what
  belongs together, especially when connecting multiple commands in onr
  \texttt{RUN} with \texttt{\&\&}
\item
  use \texttt{\textbackslash{}} for newlines
\item
  put each dependency on it's own line, makes it easier to spot changes
  in version control
\item
  don't let lines get too long
\item
  split up an instruction (especially relevant for \texttt{RUN}) when
  you have to scroll to see all of it
\item
  use a linter to follow common practices and consistency
\item
  \textbf{Use comments to document decisions and usage}

  \begin{itemize}
  \tightlist
  \item
    make the \texttt{Dockerfile} self-explanatory by adding comments for
    specific decisions
  \item
    add reasons and links to followed tutorials (failed attempts may be
    found in the history)
  \item
    similar to ``literate programming''
  \item
    include commands that did \emph{not} work, especially if they seem
    simpler, so you're not falling into the same trap twice
  \end{itemize}
\end{itemize}

\begin{verbatim}
# apt-get install specific version

# RUN command spreading several lines
RUN install2.r \
  fortunes \
  here

# this library must be installed from source to get version newer than in sources

# following commands from instructions at LINK HERE
\end{verbatim}

\begin{itemize}
\tightlist
\item
  clarity is always more important than brevity
\item
  don't worry about image size, clarity is more important (i.e.~no
  complex RUN instructions that remove files after using rightaway)

  \begin{itemize}
  \tightlist
  \item
    make \texttt{RUN} instructions do one thing (e.g.~install one
    software, install many softwares from \emph{one} source, \ldots{})
  \item
    a single \texttt{RUN} instruction should not be longer then ``one
    page'' \textgreater{} no scrolling
  \end{itemize}
\item
  have commands \emph{in order} of least likely to change to most likely
  to change, it helps readers and takes advantage of build caching

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    system libraries (you can use comments to document which library is
    required by which language-specific package)
  \item
    language-specific libraries or modules
  \item
    from repositories (binaries)
  \item
    from source
  \item
    own software/scripts (if not mounted)
  \item
    labels
  \item
    \texttt{RUN}/\texttt{ENTRYPOINT}
  \end{enumerate}
\item
  how to access the layer commands from an existing image
  (\texttt{docker\ inspect..})
\item
  Only switch directoryies with \texttt{WORKDIR} \{-\}

  \begin{itemize}
  \tightlist
  \item
    might need to move to different directories for bespoke
    configuration or building from source
  \item
    is is much more transparent than \texttt{cd\ X} or \texttt{cd\ ...}
    in \texttt{RUN} statements
  \end{itemize}
\item
  if need be use \emph{layered builds} to only keep specific files from
  one build step to another, e.g.~for build dependencies if building
  software from source
\end{itemize}

\hypertarget{pin-versions}{%
\section*{4. Pin versions}\label{pin-versions}}
\addcontentsline{toc}{section}{4. Pin versions}

\textbf{system libraries}

\begin{itemize}
\tightlist
\item
  you can install specific versions of system packages with the
  respective package manager, also called version pinning

  \begin{itemize}
  \tightlist
  \item
    on apt: https://blog.backslasher.net/my-pinning-guidelines.html
  \end{itemize}
\item
  do so if the version is relevant, e.g.~to demonstrate a bug, or likely
  to become a problem, e.g.~because of \ldots{}
\item
  do so if you are aware of the system library being relevant to your
  workflow
\item
  you can find out about the currently installed versions

  \begin{itemize}
  \tightlist
  \item
    Debian/Ubuntu: \texttt{dpkg\ -\/-list}
  \item
    Alpine: \texttt{apk\ -vv\ info\textbar{}sort}
  \item
    CentOS: \texttt{yum\ list\ installed} or \texttt{rpm\ -qa}
  \end{itemize}
\item
  \emph{installing from source} is a useful way to install very specific
  versions, at the cost of needing build libraries (which could be
  removed again with build stages, but that's advanced)
\end{itemize}

\textbf{extension packages and programming language modules}

\begin{itemize}
\tightlist
\item
  package managers of programming languages are a good solution to
  install a collection of dependencies for a language
\item
  package managers have a CLI and can be used from \texttt{RUN} commands
\item
  freezing the environment,
  cf.~https://markwoodbridge.com/2017/03/05/jupyter-reproducible-science.html
  cited by {[}2{]}
\item
  there is a risk in outsourcing configuration to the file formats
  supported by package managers \textgreater{} use only when direct
  installation in Dockerfile becomes complex; example files:

  \begin{itemize}
  \tightlist
  \item
    Python: \texttt{requirements\ .txt}, \texttt{xxx.yml} (Conda)
  \item
    R: \texttt{DESCRIPTION}
  \item
    Java: \texttt{mvn.xml}
  \item
    JavaScript: \texttt{package.json} of \texttt{npm}
  \end{itemize}
\item
  how to do in Python (\texttt{==\ x.y.z})
\item
  do it in R with \texttt{versions} package, or by using MRAN (e.g.~via
  \texttt{r-ver} image)- JavaScript?
\item
  Julia: \texttt{add\ Package@1.0} \textgreater{}
  https://julialang.github.io/Pkg.jl/v1/managing-packages/\#Adding-packages-1
\item
  Use common command-line ready installation commands of programming
  languages

  \begin{itemize}
  \tightlist
  \item
    better readbiliy, potentially even performance
    (\texttt{RUN\ install2.r\ sp} instead of
    \texttt{RUN\ R\ -e\ "install.packages(\textquotesingle{}sf\textquotesingle{})"},
    although the latter is ``base R'')
  \end{itemize}
\end{itemize}

\hypertarget{mount-data-and-control-code}{%
\section*{5. Mount data and control
code}\label{mount-data-and-control-code}}
\addcontentsline{toc}{section}{5. Mount data and control code}

\begin{itemize}
\tightlist
\item
  do not use \texttt{ADD}/\texttt{COPY} to insert data or code into an
  image
\item
  better mount them to have them outside of the image
\item
  easier access, does not require Docker knowledge by third parties to
  reause code and data
\item
  mounting ensures you are always ready to throw containers and even
  images away
\item
  use the \texttt{-\/-user} option to avoid problems with file
  permissions when mounting
\item
  if you have a ``stable'' published software library, install it from
  source from the source code repo or from the software repository (so
  that users find the project in the future)
\item
  how to mount the data can be put in the example commands (see 4.)
\item
  prefer the long versions of CLI parameters for readability
\end{itemize}

\begin{verbatim}
docker run --volume ...
\end{verbatim}

\hypertarget{capture-environment-metadata}{%
\section*{6. Capture environment
metadata}\label{capture-environment-metadata}}
\addcontentsline{toc}{section}{6. Capture environment metadata}

Use labels and build arguments for metadata

\begin{itemize}
\tightlist
\item
  \textbf{labels}

  \begin{itemize}
  \tightlist
  \item
    advantage of labels: are structured metadata, can be exposed by
    APIs, e.g.~https://microbadger.com/labels
  \item
    use namespaced-names in labels (http://label-schema.org/rc1/
    respectively https://github.com/opencontainers/image-spec)

    \begin{itemize}
    \tightlist
    \item
      should we use the article to establish some core metadata fields
      for research? author DOI? research organisations
      (https://ror.org/)? funding agency/grant number?
    \end{itemize}
  \item
    are part of the exported Docker image, mention
    \texttt{docker\ inspect}
  \item
    important metadata items

    \begin{itemize}
    \tightlist
    \item
      repository link where Dockerfile is published
    \item
      author (\texttt{MAINTAINER} is deprecated) and contact
      (e.g.~email, project website)
    \item
      license
    \item
      usage instructions?
    \item
      DOI of research compendium (Zenodo preregister instead of GitHub
      automatic integration?)
    \end{itemize}
  \item
    can https://codemeta.github.io/ and
    https://citation-file-format.github.io/ be used/useful?
  \end{itemize}
\item
  \textbf{build arguments}

  \begin{itemize}
  \tightlist
  \item
    use build arguments to capture build metadata
  \item
    git commit hash
  \item
    date and timestamp
  \item
    clarifies if build was automated
  \end{itemize}
\end{itemize}

\hypertarget{enable-interactive-usage-and-one-click-execution}{%
\section*{7. Enable interactive usage and one-click
execution}\label{enable-interactive-usage-and-one-click-execution}}
\addcontentsline{toc}{section}{7. Enable interactive usage and one-click
execution}

\begin{itemize}
\tightlist
\item
  using \texttt{CMD} and \texttt{ENTRYPOINT} make sure that it is
  possible to run the container interactively \emph{and} as a one-click
  execution \textgreater{} give examples (see below)
\item
  the default execution should either execute the workflow (headless) or
  start an analysis environment

  \begin{itemize}
  \tightlist
  \item
    if your workflow/sofware does not support headless execution
    (Excel?), switch tools
  \item
    or have default with UI and only document headless execution via
    example commands
  \end{itemize}
\item
  may also use the same \texttt{Dockerfile} for different purposes,
  e.g.~include an app (e.g.~Shiny) for interactive use by user
\item
  if you want to expose a user interface \textbf{use the browser} on and
  exposed port, unless you're using an existing Desktop you, then you
  can use \texttt{x11docker} {[}{]}
\item
  one useful alternative: Notebook user interfaces in the browser
  (Jupyter, RStudio)
\item
  document both variants with example commands
\item
  a headless execution can be executed in a continuous integration (CI)
  after every project update, potentially on a test dataset for
  speed-up,
\item
  see also Rule 7: Build a pipeline in {[}2{]}
\item
  you can also make your workflow configurable, e.g.~by bespoke
  configuration files, environment variables passed to the container
  {[}15{]}, or special Docker-based wrappers such as Kliko {[}16{]};
  however, know this is a trade-off from plain \texttt{Dockerfile}-based
  solutions, which is a proven industry standard
\item
  \emph{what user should run within the Dockerfile?}
\end{itemize}

\hypertarget{end-the-dockerfile-with-build-and-run-commands}{%
\section{\texorpdfstring{8. End the \texttt{Dockerfile} with build and
run
commands}{8. End the Dockerfile with build and run commands}}\label{end-the-dockerfile-with-build-and-run-commands}}

\begin{itemize}
\tightlist
\item
  put \texttt{docker\ run} and \texttt{docker\ build} commands in
  comments at the end of the file (\emph{may be own rule?}), especially
  relevant if arguments are used such as port exposure or mount points
\item
  should be copy-and-pasteable
\item
  use multiple comment characters to make clear what is command and what
  is documentation
\end{itemize}

\begin{verbatim}
# Build the images with
##> docker build --tag great_workflow .
# Run the image:
##> docker run --it --port 80:80 --volume ./input:/input --name gwf great_workflow
# Extract the data:
##> docker cp gwf:/output/ ./output
\end{verbatim}

\hypertarget{publish-a-dockerfile-per-project-in-a-code-repository-with-version-control}{%
\section*{9. Publish a Dockerfile per project in a code repository with
version
control}\label{publish-a-dockerfile-per-project-in-a-code-repository-with-version-control}}
\addcontentsline{toc}{section}{9. Publish a Dockerfile per project in a
code repository with version control}

\begin{itemize}
\tightlist
\item
  \texttt{Dockerfile} is a plain text-based format and therefore you
  should put it under version control
\item
  just as code, the \texttt{Dockerfile} should be for humans to
  understand and just incidentally for machines to be interpretable
  (qoute Abelson? see also
  https://www.quora.com/How-true-is-Programs-are-meant-to-be-read-by-humans-and-only-incidentally-for-computers-to-execute)
\item
  using and publishing a \texttt{Dockerfile} to create a container will
  increase chances of preservation (cf. {[}17{]})
\item
  Consult Ten Simple Rules paper by Perez-Riverol et al. {[}18{]}
\item
  add the link to the online repository to a label, to point back to the
  source of the file
\item
  versioning on a collaboration platform exposes your environment
  configuration and enables collaboration/feedback
\item
  you can build and run (e.g.~on a test dataset!) you Dockerfile in CI
  (cf.~automation below)
\item
  keep \texttt{Dockerfile} in the same project with your workflow and
  data (cf.~research compendium concept?)
\item
  \textbf{this should be the repository with the workflow and data}
  (cf.~research compendium)

  \begin{itemize}
  \tightlist
  \item
    Use one \texttt{Dockerfile} per workflow or project and put one
    ``thing'' in; \textbf{TO DISCUSS}: argue against the above rule and
    recommend having a process manager and multiple processes in one
    container
  \item
    start with a clean slate for a new project - shared lines are
    quickly copied over, and Docker's build caching will bring some
    performance
  \item
    allows you to quickly switch between projects and not worry about
    breaking things you are not working on
  \item
    have one obvious main process per project, e.g. \texttt{R} or
    RStudio
  \item
    if you have a complex set-up of several proecceses, e.g.~with a
    database, then put it in a separate container and connect them via
    \texttt{docker-compose}
  \end{itemize}
\item
  use git commit messages extensively to describe the reasons behind
  changes; the messages may even contain failed attempts/commands
\item
  add a clear license
\item
  publish the image of the workflow to a suitable repository (where it
  gets a DOI) at the time of publication of the workflow
\item
  you may use multiple containers and \texttt{Dockerfiles} for complex
  workflows (cf. {[}19{]}) but then you're probably out of scope of this
  article
\end{itemize}

\hypertarget{use-the-container-daily-rebuild-the-image-weekly}{%
\section*{10. Use the container daily, rebuild the image
weekly}\label{use-the-container-daily-rebuild-the-image-weekly}}
\addcontentsline{toc}{section}{10. Use the container daily, rebuild the
image weekly}

\begin{itemize}
\tightlist
\item
  use the container built by the \texttt{Dockerfile} in your regular
  work, it is the only way to make sure it is really stable
  (cf.~Marwick's ``this container is the only way I have ever run this
  workflow'')
\item
  no showstopper for using UIs (web-based, e.g.~Jupyter, RStudio, but
  also \texttt{x11docker})
\item
  you cannot expect to take a year old Docker image form the shelf and
  that can be extended, it will likely ``run'' but just as-is
  \textgreater{} need to re-build ``all the time'' to stay reusable; the
  longer you wait with trying to recompile the image the harder it will
  get (you don't know for which of the different reasons it fails)
\item
  during development and analysis, interactive use (e.g.~R session,
  Jupyter Notebook) has advantages, and even the most disciplined might
  install a package or change a parameter manually
\item
  regularly delete all containers and rebuild images based on your
  \texttt{Dockerfile}
\item
  you are more likely to remember the undocumented steps if done
  regularly
\item
  increases trust in configuration, encourages effetiveness and fully
  scripted configuration
\item
  keep a \texttt{Makefile} next to the Dockerfile so you don't fall into
  the trap of not regularly rebuilding your digital laboratory (better
  to have build and run commands - i.e.~the usage - in two places and
  potentially diverging than the actual \texttt{Dockerfile})
\item
  \textbf{Don't replicate environment configuration outside of the
  Dockerfile for convenience}

  \begin{itemize}
  \tightlist
  \item
    make the Dockerfile work for your day-to-day research instead of
    having a second set of configurations in on the ``local'' machine
  \item
    having two approaches will eventually break, only a perceived
    convenience
  \item
    avoid an untidy laboratory in practice behind a shiny appearance of
    a \texttt{Dockerfile}
  \item
    you can install interactive UIs as part of the Dockerfile and use
    them just like Desktop UIs (Jupyter, RStudio, use )
  \end{itemize}
\item
  if you can, get a colleague to run the workflow for you, or even
  better switch Dockerfiles and give feedback - this gives both of you
  an extra layer of confidence
\item
  add a CRON job that deletes the image every week?
\end{itemize}

\textbf{Box: Automatic generation of Dockerfiles}

\begin{itemize}
\tightlist
\item
  there are tools you can auto-generate a \texttt{Dockerfile}
\item
  can be a good as a starting point, careful to avoid a lock-in
\item
  they have limitations, namely \ldots{}
\item
  \texttt{repo2docker}, \texttt{dockter}, \texttt{containerit}
\item
  these are useful if you don't need very specific versions etc. and for
  specific use cases, but sometimes requires a specific project
  structure (PyPI \texttt{requirements.txt}) or reproducible document (R
  Markdown file)
\end{itemize}

\hypertarget{example-dockerfiles}{%
\section{Example Dockerfiles}\label{example-dockerfiles}}

To demonstrate the 10 rules, we have a git repository with example
\texttt{Dockerfile}s, some of which we took from public repositories and
updated to adhere to the rules (\texttt{Dockerfile.before} and
\texttt{Dockerfile.after}).

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

Reproducibility in research is an endeavour of best efforts, not about
achieving the perfect solution, as that is probably not achievable or
changing over time. This article provides guidance for using
\texttt{Dockerfile}s in computational/computer-based research to work
towards a ``time capsule'' (see
https://twitter.com/DougBlank/status/1135904909663068165?s=09) that
given some expertise and the right tools can be used to come as close as
possible to the original virtual laboratory used for a specific. Such an
increase in transparency and conscious effort is valuable for the
creators of analyses, even if the capsule decays over time. The effort
should also be valued by others and may change the way scholars
collaborate and communicate (cf.~notion of ``preproducibility'' by
{[}20{]}) So please, don't go insane with writing \texttt{Dockerfile}s,
but be realistic about what might break and what is unlikely to break.
In a similar vein, these rules may be broken if another way works better
for \emph{your use case}. The rules in this article help you mastering
the \texttt{Dockerfile} format for research and provide a solid basis
for engaging in more complex but also in simpler assisted usage of
containers (cf.~Box: Aumatic Generation). Corner cases aside, share and
exchange your \texttt{Dockerfile} freely and collaborate in your
community to spread the knowledge about containers as a tool for
research. Together you can develop common practices or even shared base
images (exemplified by communities listed in Rule 1).

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

o2r by DFG

\hypertarget{contributions}{%
\section*{Contributions}\label{contributions}}
\addcontentsline{toc}{section}{Contributions}

DN conceived the idea, wrote the first darft, contributed to all rules.
SE contributed to the overall structure and selected rules. TH
contributed to the rule structure and particularly rule 1.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-boettiger_introduction_2017}{}%
1. Boettiger C, Eddelbuettel D. An Introduction to Rocker: Docker
Containers for R. The R Journal. 2017;9: 527--536.
doi:\href{https://doi.org/10.32614/RJ-2017-065}{10.32614/RJ-2017-065}

\leavevmode\hypertarget{ref-rule_ten_2019}{}%
2. Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et
al. Ten simple rules for writing and sharing computational analyses in
Jupyter Notebooks. PLOS Computational Biology. 2019;15: e1007007.
doi:\href{https://doi.org/10.1371/journal.pcbi.1007007}{10.1371/journal.pcbi.1007007}

\leavevmode\hypertarget{ref-sandve_ten_2013}{}%
3. Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for
Reproducible Computational Research. PLoS Comput Biol. 2013;9: e1003285.
doi:\href{https://doi.org/10.1371/journal.pcbi.1003285}{10.1371/journal.pcbi.1003285}

\leavevmode\hypertarget{ref-brinckman_computing_2018}{}%
4. Brinckman A, Chard K, Gaffney N, Hategan M, Jones MB, Kowalik K, et
al. Computing environments for reproducibility: Capturing the ``Whole
Tale''. Future Generation Computer Systems. 2018;
doi:\href{https://doi.org/10.1016/j.future.2017.12.029}{10.1016/j.future.2017.12.029}

\leavevmode\hypertarget{ref-code_ocean_2019}{}%
5. Code Ocean {[}Internet{]}. 2019. Available:
\url{https://codeocean.com/}

\leavevmode\hypertarget{ref-simko_reana_2019}{}%
6. Å imko T, Heinrich L, Hirvonsalo H, Kousidis D, RodrÃ­guez D. REANA: A
System for Reusable Research Data Analyses. EPJ Web of Conferences.
2019;214: 06034.
doi:\href{https://doi.org/10.1051/epjconf/201921406034}{10.1051/epjconf/201921406034}

\leavevmode\hypertarget{ref-jupyter_binder_2018}{}%
7. Jupyter P, Bussonnier M, Forde J, Freeman J, Granger B, Head T, et
al. Binder 2.0 - Reproducible, interactive, sharable environments for
science at scale. Proceedings of the 17th Python in Science Conference.
2018; 113--120.
doi:\href{https://doi.org/10.25080/Majora-4af1f417-011}{10.25080/Majora-4af1f417-011}

\leavevmode\hypertarget{ref-nust_opening_2017}{}%
8. NÃ¼st D, Konkol M, Pebesma E, Kray C, Schutzeichel M, Przibytzin H, et
al. Opening the Publication Process with Executable Research Compendia.
D-Lib Magazine. 2017;23.
doi:\href{https://doi.org/10.1045/january2017-nuest}{10.1045/january2017-nuest}

\leavevmode\hypertarget{ref-chen_open_2019}{}%
9. Chen X, Dallmeier-Tiessen S, Dasler R, Feger S, Fokianos P, Gonzalez
JB, et al. Open is not enough. Nature Physics. 2019;15: 113.
doi:\href{https://doi.org/10.1038/s41567-018-0342-2}{10.1038/s41567-018-0342-2}

\leavevmode\hypertarget{ref-donoho_invitation_2010}{}%
10. Donoho DL. An invitation to reproducible computational research.
Biostatistics. 2010;11: 385--388.
doi:\href{https://doi.org/10.1093/biostatistics/kxq028}{10.1093/biostatistics/kxq028}

\leavevmode\hypertarget{ref-kurtzer_singularity_2017}{}%
11. Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers
for mobility of compute. PLOS ONE. 2017;12: e0177459.
doi:\href{https://doi.org/10.1371/journal.pone.0177459}{10.1371/journal.pone.0177459}

\leavevmode\hypertarget{ref-boettiger_introduction_2015}{}%
12. Boettiger C. An Introduction to Docker for Reproducible Research.
SIGOPS Oper Syst Rev. 2015;49: 71--79.
doi:\href{https://doi.org/10.1145/2723872.2723882}{10.1145/2723872.2723882}

\leavevmode\hypertarget{ref-marwick_madjebebe_2015}{}%
13. Ben Marwick. 1989-excavation-report-Madjebebe. 2015;
doi:\href{https://doi.org/10.6084/m9.figshare.1297059}{10.6084/m9.figshare.1297059}

\leavevmode\hypertarget{ref-halchenko_open_2012}{}%
14. Halchenko YO, Hanke M. Open is Not Enough. Let's Take the Next Step:
An Integrated, Community-Driven Computing Platform for Neuroscience.
Frontiers in Neuroinformatics. 2012;6.
doi:\href{https://doi.org/10.3389/fninf.2012.00022}{10.3389/fninf.2012.00022}

\leavevmode\hypertarget{ref-knoth_reproducibility_2017}{}%
15. Knoth C, NÃ¼st D. Reproducibility and Practical Adoption of GEOBIA
with Open-Source Software in Docker Containers. Remote Sensing. 2017;9:
290. doi:\href{https://doi.org/10.3390/rs9030290}{10.3390/rs9030290}

\leavevmode\hypertarget{ref-molenaar_klikoscientific_2018}{}%
16. Molenaar G, Makhathini S, Girard JN, Smirnov O. Kliko---The
scientific compute container format. Astronomy and Computing. 2018;25:
1--9.
doi:\href{https://doi.org/10.1016/j.ascom.2018.08.003}{10.1016/j.ascom.2018.08.003}

\leavevmode\hypertarget{ref-emsley_framework_2018}{}%
17. Emsley I, De Roure D. A Framework for the Preservation of a Docker
Container International Journal of Digital Curation. International
Journal of Digital Curation. 2018;12.
doi:\href{https://doi.org/10.2218/ijdc.v12i2.509}{10.2218/ijdc.v12i2.509}

\leavevmode\hypertarget{ref-perez-riverol_ten_2016}{}%
18. Perez-Riverol Y, Gatto L, Wang R, Sachsenberg T, Uszkoreit J,
Leprevost F da V, et al. Ten Simple Rules for Taking Advantage of Git
and GitHub. PLOS Computational Biology. 2016;12: e1004947.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004947}{10.1371/journal.pcbi.1004947}

\leavevmode\hypertarget{ref-kim_bio-docklets_2017}{}%
19. Kim B, Ali TA, Lijeron C, Afgan E, Krampis K. Bio-Docklets:
Virtualization Containers for Single-Step Execution of NGS Pipelines.
bioRxiv. 2017; 116962.
doi:\href{https://doi.org/10.1101/116962}{10.1101/116962}

\leavevmode\hypertarget{ref-stark_before_2018}{}%
20. Stark PB. Before reproducibility must come preproducibility
{[}Internet{]}. Nature. 2018.
doi:\href{https://doi.org/10.1038/d41586-018-05256-0}{10.1038/d41586-018-05256-0}

\nolinenumbers


\end{document}

